---
title: "Speciale_analysis"
author: "Martin Andersen"
date: "2023-02-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r,message=FALSE}
# Load packages:
library(tidyverse)
library(lubridate)
library(readxl)
library(timetk)
library(dplyr)
library(zoo)
library(furrr)
library(ggplot2)


source("functions.R")
```


################ Portfolio sorts 1 week sentiment data:
First we download the data that we want to work with, and compute the market cap weighted mean
################
```{r}

# Data connection to get STOXX 600 Europe sentiment and stock data. Joined on stock data, so there will be NA's in the sentiment.  
data_weekly <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/all_data_weekly.csv", sep = ",") %>% 
  select(-c(X,observations_unique_sentences,observations_unique_articles,observations_unique_sources,global_unique_sentences,global_unique_articles,global_unique_sources,
            sentiment_negative_mean:sdg_broad_positive_count
            )) %>% 
  na.omit(observations) %>%
  group_by(date) %>% 
  mutate(scalar= round(1 + scale(MKT_CAP),2),
         negative_sum = round((negative_sum/scalar),2),
         neutral_sum = round((neutral_sum/scalar),2), 
         positive_sum = round((positive_sum/scalar),2)
         ) %>% arrange(date) 

# Data connection to get daily market returns for STOXX 600 Europe
msci_index_daily <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/MSCI_World_daily.csv", sep = ",") %>%
  mutate(date = as.Date(Date, format = "%m/%d/%Y"),
    mkt_excess = as.numeric(sub("%", "",Change..))/100)

```


################ 
Univariate portfolio sorts 1 week sentiment data:
################
```{r}

sentiment_portfolios <- data_weekly %>% # filter(positive_sum > 0 | negative_sum > 0) %>%
  group_by(date) %>%
  mutate(
    breakpoint = mean(negative_sum),
    portfolio = case_when(
      negative_sum <= breakpoint ~ "low",
      negative_sum > breakpoint ~ "high"
    )
  ) |>
  group_by(date, portfolio) |>
  summarize(ret = weighted.mean(W_RETURN, MKT_CAP), .groups = "drop") %>%
  pivot_wider(date, names_from = portfolio, values_from = ret) |>
  mutate(ret = low - high)

portfolios_summary <- beta_portfolios %>% mutate(date = as.Date(date)) %>%
  left_join(stoxx600_index %>% select(date,mkt_excess), by = "date") %>%
  #group_by(portfolio) |>
  summarize(
    alpha = as.numeric(lm(ret ~ 1 + mkt_excess)$coefficients[1]),
    beta = as.numeric(lm(ret ~ 1 + mkt_excess)$coefficients[2]),
    ret = 54*mean(mkt_excess)
  )

```

################
Bivariate sorts
################
```{r}

value_portfolios <- data_weekly %>% filter(positive_sum > 0 | negative_sum > 0) %>%
  group_by(date) %>%
  
  mutate(portfolio_pos = assign_portfolio_bi(
    data = cur_data(),
    var = positive_sum,
    n_portfolios = 3
  )) %>%
  
  group_by(date, portfolio_pos) %>%
  mutate(
    portfolio_neg = assign_portfolio_bi(
      data = cur_data(),
      var = negative_sum,
      n_portfolios = 3
    ),
    
    portfolio_combined = str_c(portfolio_neg, portfolio_pos)
  ) %>% 
  
  select(date,isin,portfolio_neg,portfolio_pos,portfolio_combined,W_RETURN,MKT_CAP,negative_sum) %>% filter(portfolio_neg == 1)
  group_by(date, portfolio_combined) %>%
  summarize(
    ret = weighted.mean(W_RETURN, MKT_CAP),
    portfolio_pos = unique(portfolio_pos),
    .groups = "drop"
  )

value_portfolios %>% 
  group_by(portfolio_combined) %>%
  summarise(ret = mean(ret))

# 
# value_premium <- value_portfolios %>%
#   group_by(date, portfolio_pos) %>%
#   summarize(ret = mean(ret), .groups = "drop_last") %>%
#   summarize(value_premium = ret[portfolio_pos == max(portfolio_pos)] -
#     ret[portfolio_pos == min(portfolio_pos)])
# 
# mean(value_premium$value_premium * 100)
  short_term_data_event %>%
  filter(pos_event == 1)
```



################
Short term abnormal return after an individual event - Daily
################
```{r}
data_daily <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/data_daily.csv", sep = ",")  %>% 
  select(date,isin,W_RETURN,negative_sum,positive_sum) %>%
  mutate(date = as.Date(date)) %>% 
  right_join(msci_index_daily %>% select(date, mkt_excess), by = "date")
  

plan(multisession, workers = availableCores())

short_term_data_nested <- data_daily %>% ungroup() %>% 
  select(date,isin,W_RETURN,mkt_excess) %>%
  nest(data = -c(isin)) %>%
  mutate(beta = future_map(
    data, ~ roll_capm_estimation(., window = 120, min_obs = 60, period = "day")
  )) %>%
  unnest(c(beta)) %>%
  na.omit(beta,alpha) %>% select(-data)

# # Write to CSV to keep in nice format:
        # short_term_data_nested %>%
        #   write.csv(file = "C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/nested_daily_for_capm.csv")

 short_term_data_nested <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/nested_daily_for_capm.csv", sep = ",") %>%
   select(-X) %>%
   mutate(date = as.Date(date))
 
 sentiment_daily <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/clean_sentiment_daily.csv", sep = ",") %>%
   select(-X) %>%
   mutate(date = as.Date(date))

 
 
short_term_data <- short_term_data_nested %>% filter(isin == 'SE0017768716') %>%
  left_join(data_daily, by = c("date","isin")) %>%
  
  left_join(
    sentiment_daily %>%
      mutate(date = as.Date(date)) %>%
      select(date,isin,sdg_not_relevant_negative_count,sdg_not_relevant_positive_count,sentiment_negative_count,sentiment_positive_count), 
            by =c("date","isin")) %>%
  
  group_by(isin) %>%
  mutate(
      pred_ret = alpha + beta*mkt_excess,
      abnormal_ret = W_RETURN - pred_ret,
      norm_positive_sum = sentiment_positive_count - sdg_not_relevant_positive_count,
      norm_negative_sum = sentiment_negative_count - sdg_not_relevant_negative_count,
      
      roll_sum = across(!c(date,sdg_not_relevant_positive_count,sdg_not_relevant_negative_count,beta,alpha,W_RETURN,negative_sum,
                      positive_sum,mkt_excess,pred_ret,abnormal_ret,sentiment_positive_count,sentiment_negative_count),
                      ~ rollsum(.x, k = 5, fill = NA, align = "right"))) %>% 
  unnest(roll_sum,names_sep = "_") %>%
  
  mutate(
    roll_mean = across(!c(date,sdg_not_relevant_positive_count,sdg_not_relevant_negative_count,beta,alpha,W_RETURN,negative_sum,
                       positive_sum,mkt_excess,pred_ret,abnormal_ret,roll_sum_norm_positive_sum,roll_sum_norm_negative_sum,
                       sentiment_positive_count,sentiment_negative_count),
                      ~ rollmean(.x, k = 5, fill = NA, align = "right"))
    
     
  ) %>%
  unnest(roll_mean,names_sep = "_") %>%
  select(-c(beta,alpha,pred_ret,sdg_not_relevant_positive_count,sdg_not_relevant_negative_count))

# Create columns that represent the lagged and leading values in relation to the event date:

for (i in 1:15) {
  short_term_data[paste0("-",i)] <- short_term_data %>% group_by(isin) %>% transmute(t = lag(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
  short_term_data[paste0("+",i)] <- short_term_data %>% group_by(isin) %>% transmute(t = lead(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
}

short_term_data_event = short_term_data %>% 
  group_by(isin) %>%
  mutate(
    
    norm_positive_sum = roll_mean_norm_positive_sum,
    norm_negative_sum = roll_mean_norm_negative_sum,
    
    norm_positive_sum = na_if(norm_positive_sum, 0),
    norm_negative_sum = na_if(norm_negative_sum, 0),
    
    positive_sd = sd(norm_positive_sum,na.rm = TRUE),
    negative_sd = sd(norm_negative_sum,na.rm = TRUE),
    
    positive_threshold = mean(norm_positive_sum,na.rm = TRUE) + 2*positive_sd,
    negative_threshold = mean(norm_negative_sum,na.rm = TRUE) + 2*negative_sd
    ) %>%
  
   select(-c("W_RETURN","mkt_excess","positive_sum","negative_sum","positive_sd","negative_sd",
             #"positive_threshold","negative_threshold","norm_positive_sum","norm_negative_sum",
            "sentiment_negative_count","sentiment_positive_count","roll_mean_norm_negative_sum","roll_mean_norm_positive_sum",
            "roll_sum_norm_negative_sum","roll_sum_norm_positive_sum")) %>%
  rename("0" = abnormal_ret) %>%

  group_by(date,isin) %>%
  # Make sure that we only calculate the cutoff value of periods with actual observations. 
  mutate(pos_event = 
    case_when(norm_positive_sum > positive_threshold & norm_positive_sum > 10 & norm_positive_sum > 2*norm_negative_sum ~ 1,
              TRUE
               ~ 0),
    neg_event = 
    case_when(norm_negative_sum > negative_threshold & norm_negative_sum > 10 & norm_negative_sum > 2*norm_positive_sum ~ 1, TRUE
               ~ 0)
    ) 

positive_index = short_term_data_event %>% 
  filter(pos_event == 1) %>%
  select(-c(pos_event,neg_event,positive_threshold,negative_threshold,norm_positive_sum,norm_negative_sum)) %>%
  pivot_longer(!c(date,isin), names_to = "type", values_to = "value") %>%
  na.omit(value) %>%
  mutate(type = as.factor(type)) %>%
  group_by(type) %>% 
  summarise(AAR = mean(value)) %>%
  mutate(event = "positive",
         type = factor(type, levels = c("-15","-14","-13","-12","-11","-10", "-9","-8","-7","-6","-5","-4","-3","-2","-1","0", "+1", "+2","+3","+4","+5","+6","+7","+8","+9","+10","+11", "+12","+13","+14","+15"))) %>%
  arrange(type) %>%
  mutate(CAAR = cumsum(AAR))
         
negative_index = short_term_data_event %>%
  filter(neg_event == 1)  %>% 
  select(-c(pos_event,neg_event,positive_threshold,negative_threshold,norm_positive_sum,norm_negative_sum)) %>%
  pivot_longer(!c(date,isin), names_to = "type", values_to = "value") %>%
  na.omit(value) %>%
  mutate(type = as.factor(type)) %>%
  group_by(type) %>% 
  summarise(AAR = mean(value)) %>%
  mutate( event = "negative",
    type = factor(type, levels = c("-15","-14","-13","-12","-11","-10", "-9","-8","-7","-6","-5","-4","-3","-2","-1","0", "+1", "+2","+3","+4","+5","+6","+7","+8","+9","+10","+11", "+12","+13","+14","+15"))) %>%
  arrange(type) %>%
  mutate(CAAR = cumsum(AAR))

positive_index %>% 
  pivot_longer(!c(type,event), names_to = "metric", values_to = "value") %>%
  ggplot(aes(x = type, y = value, colour = metric, group = metric)) + 
  geom_line() +
  theme(legend.title = element_blank())
ggsave("ST_positive_all.png")

negative_index %>% 
  pivot_longer(!c(type,event), names_to = "metric", values_to = "value") %>%
  ggplot(aes(x = type, y = value, colour = metric, group = metric)) + 
  geom_line() +
  theme_bw() +
  theme(legend.title = element_blank()) 
ggsave("ST_negative_all.png")

  
#   data = short_term_data_event %>%
#   filter(pos_event == 1) %>% na.omit(1)
#   
#  data = data[!(is.na(data$'0')), ]
#   
# quartiles <- quantile(data$'0', probs=c(.25, .75), na.rm = TRUE)
# IQR <- IQR(data$'0')
#  
# Lower <- quartiles[1] - 1.5*IQR
# Upper <- quartiles[2] + 1.5*IQR 
#  
# data_no_outlier <- subset(data, data$'0' > Lower & data$'0' < Upper)
#  dim(data)
# dim(data_no_outlier)


```


################
Short term abnormal return after an individual event - Daily split on SDG's
################
```{r}
data_daily <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/data_daily.csv", sep = ",")  %>% 
  select(date,isin,W_RETURN,negative_sum,positive_sum) %>%
  mutate(date = as.Date(date)) %>% 
  right_join(msci_index_daily %>% select(date, mkt_excess), by = "date")

 short_term_data_nested <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/nested_daily_for_capm.csv", sep = ",") %>%
   select(-X) %>%
   mutate(date = as.Date(date))
 
 sentiment_daily <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/clean_sentiment_daily.csv", sep = ",") %>%
   select(-X) %>%
   mutate(date = as.Date(date))

short_term_data <- short_term_data_nested %>% 
  left_join(data_daily, by = c("date","isin")) %>%
  
  left_join(
    sentiment_daily %>%
      mutate(date = as.Date(date)),
      # select(date,isin,sdg_not_relevant_negative_count,sdg_not_relevant_positive_count,sentiment_negative_count,sentiment_positive_count), 
            by = c("date","isin")) %>%
  
  group_by(isin) %>%
  mutate(
      pred_ret = alpha + beta*mkt_excess,
      abnormal_ret = W_RETURN - pred_ret
  ) %>%
  select(-c(beta,alpha,pred_ret,sdg_not_relevant_positive_count,sdg_not_relevant_negative_count))


short_term_data_event = short_term_data %>% 
  select(isin,date,abnormal_ret, contains("sdg")) %>% 
  select(isin,date,abnormal_ret, contains("tive"),-c(sdg_broad_negative_count,sdg_broad_neutral_count,sdg_broad_positive_count))

# Create columns that represent the lagged and leading values in relation to the event date:
for (i in 1:15) {
  short_term_data_event[paste0("-",i)] <- short_term_data_event %>% group_by(isin) %>% transmute(t = lag(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
  short_term_data_event[paste0("+",i)] <- short_term_data_event %>% group_by(isin) %>% transmute(t = lead(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
}


short_term_return = short_term_data_event %>%
  pivot_longer(!c(date,isin,abnormal_ret,'-1':'+15'), names_to = "type", values_to = "value") %>%
  group_by(isin,type) %>%
  mutate(
      value = na_if(value, 0),
      sd = sd(value,na.rm = TRUE),
      mean = mean(value, na.rm = TRUE),
      
    threshold = mean + 1*sd
  ) %>% 
  mutate(event = 
    case_when(value > threshold & value > 1 ~ 1,
              TRUE 
               ~ 0)
    ) %>%
  rename('0' = abnormal_ret) %>%
  select(-c(value,sd,mean,threshold)) %>% 
  filter(event == 1) %>%
  mutate(group = case_when(grepl("pos", type) ~ "positive",
                            grepl("nega", type) ~"negative"),
         ) 

positive_index = short_term_return %>%
  filter(group == 'positive') %>%
  select(-c(event,group)) %>%
  pivot_longer(!c(date,isin,type), names_to = "period", values_to = "return") %>% na.omit(return) %>%
  group_by(type,period) %>%
    mutate(period = as.factor(period)) %>%
  summarise(AAR = mean(return)) %>%
  mutate( event = "positive",
    period = factor(period, levels = c("-15","-14","-13","-12","-11","-10", "-9","-8","-7","-6","-5","-4","-3","-2","-1","0", "+1", "+2","+3","+4","+5","+6","+7","+8","+9","+10","+11", "+12","+13","+14","+15"))) %>%
  arrange(type,period) %>%
  mutate(CAAR = cumsum(AAR))

positive_index %>% 
  ggplot(aes(x = period, y = CAAR, group = type, colour = type)) + 
  geom_line() +
  theme(legend.title = element_blank())
ggsave("ST_positive_sdgs.png")

negative_index = short_term_return %>%
  filter(group == 'negative') %>%
  select(-c(event,group)) %>%
  pivot_longer(!c(date,isin,type), names_to = "period", values_to = "return") %>% na.omit(return) %>%
  group_by(type,period) %>%
    mutate(period = as.factor(period)) %>%
  summarise(AAR = mean(return)) %>%
  mutate( event = "negative",
    period = factor(period, levels = c("-15","-14","-13","-12","-11","-10", "-9","-8","-7","-6","-5","-4","-3","-2","-1","0", "+1", "+2","+3","+4","+5","+6","+7","+8","+9","+10","+11", "+12","+13","+14","+15"))) %>%
  arrange(type,period) %>%
  mutate(CAAR = cumsum(AAR))
  
negative_index %>% 
   ggplot(aes(x = period, y = CAAR, group = type, colour = type)) + 
  geom_line() +
  theme(legend.title = element_blank())
ggsave("ST_negative_sdgs.png")


```



Keep working on this for the initial overview:
```{r}

long_term_daily_data <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/data_daily.csv", sep = ",")  %>% 
  select(date,isin,W_RETURN,PX_LAST,MKT_CAP,negative_sum,positive_sum) %>%
  mutate(date = as.Date(date)) %>% 
  right_join(msci_index_daily %>% select(date, mkt_excess), by = "date") %>%
   mutate(abnormal_ret = W_RETURN - mkt_excess) 

# Create columns that represent the lagged and leading values in relation to the event date:
for (i in 1:150) {
  long_term_daily_data[paste0("-",i)] <- long_term_daily_data %>% group_by(isin) %>% transmute(t = lag(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
  long_term_daily_data[paste0("+",i)] <- long_term_daily_data %>% group_by(isin) %>% transmute(t = lead(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
}

short_term_data_event = short_term_data %>% 
  group_by(isin) %>%
  mutate(
    positive_sum = na_if(positive_sum, 0),
    negative_sum = na_if(negative_sum, 0),
    positive_mean = mean(positive_sum,na.rm = TRUE),
    negative_mean = mean(negative_sum,na.rm = TRUE)) %>%
  group_by(date,isin) %>%
  # Make sure that we only calculate the cutoff value of periods with actual observations. 
  mutate(pos_event = 
    case_when(positive_sum > positive_mean ~ 1,
              TRUE
               ~ 0),
    neg_event = 
    case_when(negative_sum > negative_mean ~ 1, TRUE
               ~ 0)
    ) 

short_term_data_event %>%
  filter(pos_event == 1 | neg_event == 1) %>%
  ungroup() %>% 
  rename("0" = abnormal_ret) %>% 
  # Remove columns we don't use
  select(-c("W_RETURN","PX_LAST","mkt_excess","positive_sum","MKT_CAP","negative_sum","event","positive_mean","pos_mean")) %>%
  pivot_longer(!c(date,isin), names_to = "type", values_to = "value") %>% 
  na.omit(value) %>%
  group_by(isin,type) %>% 
  summarise(abnormal_ret = mean(value))


lagged_index = short_term_data_event %>%
  mutate(type = as.character(type),
         type = as.integer(type)) %>% filter(type <= 0) %>% 
  mutate(type = as.factor(type)) 

lagged_index$type <- factor(lagged_index$type, levels = c("0", "-1", "-2","-3","-4","-5","-6","-7","-8","-9","-10"))

lagged_index = lagged_index %>%
  arrange(isin,type) %>%
  group_by(isin) %>%
  mutate(index_ret = case_when(type == '0' ~ 0,
                        TRUE ~ abnormal_ret),
         index =  100*(cumprod(1+index_ret)))

lead_index = short_term_data_event %>%
  mutate(type = as.character(type),
         type = as.integer(type)) %>% filter(type > 0) %>% 
  mutate(type = as.factor(type)) 

lead_index$type <- factor(lead_index$type, levels = c("0", "1", "2","3","4","5","6","7","8","9","10"))

lead_index = lead_index %>%
  arrange(isin,type) %>%
  group_by(isin) %>%
  mutate(index_ret = case_when(type == '0' ~ 0,
                        TRUE ~ abnormal_ret),
         index =  100*(cumprod(1+index_ret)))

short_term_indexed = lead_index %>%
  rbind(lagged_index)

short_term_indexed$type <- factor(short_term_indexed$type, levels = c("-10", "-9","-8","-7","-6","-5","-4","-3","-2","-1","0", "1", "2","3","4","5","6","7","8","9","10"))

short_term_indexed %>% 
  ggplot(aes(x = type, y = index, colour = isin, group = isin)) + 
  geom_line() +
  theme(legend.position = "none")


short_term_indexed %>%
  group_by(type) %>%
  summarise(AAR =  mean(abnormal_ret)) %>% 
  ggplot(aes(x = type, y = AAR)) +
  geom_point() +
  theme(legend.position = "none")

```


################
Short term abnormal return after an individual event - Weekly
################
```{r}
data_weekly <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/all_data_weekly.csv", sep = ",") %>%
  select(date,isin,W_RETURN,PX_LAST,MKT_CAP,negative_sum,positive_sum) %>% 
  mutate(date = as.Date(date)) %>%
  left_join(stoxx600_index %>% select(date, mkt_excess), by = "date")

short_term_data_nested <- data_weekly %>% ungroup() %>%
  select(date,isin,W_RETURN,mkt_excess) %>%
  nest(data = -c(isin)) %>%
  mutate(beta = future_map(
    data, ~ roll_capm_estimation(.,weeks = 10, min_obs = 10)
  )) %>%
  unnest(c(beta)) %>%
  na.omit(beta,alpha) %>% select(-data)

short_term_data <- short_term_data_nested %>% 
  left_join(data_weekly, by = c("date","isin")) %>%
  mutate(
    pred_ret = alpha + beta*mkt_excess,
    abnormal_ret = W_RETURN - pred_ret) %>%
  select(-c(beta,alpha,pred_ret))

# Create columns that represent the lagged and leading values in relation to the event date:
for (i in 1:10) {
  
  short_term_data[paste0("-",i)] <- short_term_data %>% group_by(isin) %>% transmute(t = lag(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
  
  short_term_data[paste0("+",i)] <- short_term_data %>% group_by(isin) %>% transmute(t = lead(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
}


short_term_data_event = short_term_data %>%
  group_by(isin) %>%
  mutate(
    pos_mean = na_if(positive_sum, 0),
    positive_mean = mean(pos_mean,na.rm = TRUE)) %>%
  group_by(date,isin) %>%
  # Make sure that we only calculate the cutoff value of periods with actual observations. 
  mutate(event = 
    case_when(positive_sum > positive_mean ~ 1, TRUE
               ~ 0)
  ) %>%
  filter(event == 1) %>%
  ungroup() %>% 
  rename("0" = abnormal_ret) %>% 
  # Remove columns we don't use
  select(-c("W_RETURN","PX_LAST","mkt_excess","positive_sum","MKT_CAP","negative_sum","event","positive_mean","pos_mean")) %>%
  pivot_longer(!c(date,isin), names_to = "type", values_to = "value") %>% 
  na.omit(value) %>%
  group_by(isin,type) %>% 
  summarise(abnormal_ret = mean(value))


lagged_index = short_term_data_event %>%
  mutate(type = as.character(type),
         type = as.integer(type)) %>% filter(type <= 0) %>% 
  mutate(type = as.factor(type)) 

lagged_index$type <- factor(lagged_index$type, levels = c("0", "-1", "-2","-3","-4","-5","-6","-7","-8","-9","-10"))

lagged_index = lagged_index %>%
  arrange(isin,type) %>%
  group_by(isin) %>%
  mutate(index_ret = case_when(type == '0' ~ 0,
                        TRUE ~ abnormal_ret),
         index =  100*(cumprod(1+index_ret)))

lead_index = short_term_data_event %>%
  mutate(type = as.character(type),
         type = as.integer(type)) %>% filter(type > 0) %>% 
  mutate(type = as.factor(type)) 

lead_index$type <- factor(lead_index$type, levels = c("0", "1", "2","3","4","5","6","7","8","9","10"))

lead_index = lead_index %>%
  arrange(isin,type) %>%
  group_by(isin) %>%
  mutate(index_ret = case_when(type == '0' ~ 0,
                        TRUE ~ abnormal_ret),
         index =  100*(cumprod(1+index_ret)))

short_term_indexed = lead_index %>%
  rbind(lagged_index)

short_term_indexed$type <- factor(short_term_indexed$type, levels = c("-10", "-9","-8","-7","-6","-5","-4","-3","-2","-1","0", "1", "2","3","4","5","6","7","8","9","10"))

short_term_indexed %>% 
  ggplot(aes(x = type, y = index, colour = isin, group = isin)) + 
  geom_line() +
  theme(legend.position = "none")


short_term_indexed %>%
  group_by(type) %>%
  summarise(AAR =  mean(54 * abnormal_ret)) %>% 
  ggplot(aes(x = type, y = AAR)) +
  geom_point() +
  theme(legend.position = "none")
```
