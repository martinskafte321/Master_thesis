---
title: "Speciale_analysis"
author: "Martin Andersen"
date: "2023-02-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r,message=FALSE}
# Load packages:
library(tidyverse)
library(lubridate)
library(readxl)
library(timetk)
library(dplyr)
library(zoo)
library(furrr)
library(ggplot2)


source("functions.R")
```


################ Portfolio sorts 1 week sentiment data:
First we download the data that we want to work with, and compute the market cap weighted mean
################
```{r}

# Data connection to get STOXX 600 Europe sentiment and stock data. Joined on stock data, so there will be NA's in the sentiment.  
data_weekly <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/all_data_weekly.csv", sep = ",") %>% 
  select(-c(X,observations_unique_sentences,observations_unique_articles,observations_unique_sources,global_unique_sentences,global_unique_articles,global_unique_sources,
            sentiment_negative_mean:sdg_broad_positive_count
            )) %>% 
  na.omit(observations) %>%
  group_by(date) %>% 
  mutate(scalar= round(1 + scale(MKT_CAP),2),
         negative_sum = round((negative_sum/scalar),2),
         neutral_sum = round((neutral_sum/scalar),2), 
         positive_sum = round((positive_sum/scalar),2)
         ) %>% arrange(date) 

# Data connection to get Nasdaq Global sentiment data:
nasdaq_weekly <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/nasdaq_global_data_weekly.csv", sep = ",") %>% 
  select(-c(X,observations_unique_sentences,observations_unique_articles,observations_unique_sources,global_unique_sentences,global_unique_articles,global_unique_sources,
            sentiment_negative_mean:sdg_broad_positive_count)) %>% 
  na.omit(observations)

# Data connection to get market return for STOXX 600 Europe:
stoxx600_index <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/STOXX600_index.csv", sep = ",") %>%
  mutate(
    date = as.Date(Date, format = "%m/%d/%Y")-3,
    date = ceiling_date(date, "week", week_start = getOption("lubridate.week.start", 5)),
    mkt_excess = as.numeric(sub("%", "",Change..))/100)

  

```


################ 
Univariate portfolio sorts 1 week sentiment data:
################
```{r}

sentiment_portfolios <- data_weekly %>% # filter(positive_sum > 0 | negative_sum > 0) %>%
  group_by(date) %>%
  mutate(
    breakpoint = mean(negative_sum),
    portfolio = case_when(
      negative_sum <= breakpoint ~ "low",
      negative_sum > breakpoint ~ "high"
    )
  ) |>
  group_by(date, portfolio) |>
  summarize(ret = weighted.mean(W_RETURN, MKT_CAP), .groups = "drop") %>%
  pivot_wider(date, names_from = portfolio, values_from = ret) |>
  mutate(ret = low - high)

portfolios_summary <- beta_portfolios %>% mutate(date = as.Date(date)) %>%
  left_join(stoxx600_index %>% select(date,mkt_excess), by = "date") %>%
  #group_by(portfolio) |>
  summarize(
    alpha = as.numeric(lm(ret ~ 1 + mkt_excess)$coefficients[1]),
    beta = as.numeric(lm(ret ~ 1 + mkt_excess)$coefficients[2]),
    ret = 54*mean(mkt_excess)
  )

```

################
Bivariate sorts
################
```{r}

value_portfolios <- data_weekly %>% filter(positive_sum > 0 | negative_sum > 0) %>%
  group_by(date) %>%
  
  mutate(portfolio_pos = assign_portfolio_bi(
    data = cur_data(),
    var = positive_sum,
    n_portfolios = 3
  )) %>%
  
  group_by(date, portfolio_pos) %>%
  mutate(
    portfolio_neg = assign_portfolio_bi(
      data = cur_data(),
      var = negative_sum,
      n_portfolios = 3
    ),
    
    portfolio_combined = str_c(portfolio_neg, portfolio_pos)
  ) %>% 
  
  select(date,isin,portfolio_neg,portfolio_pos,portfolio_combined,W_RETURN,MKT_CAP,negative_sum) %>% filter(portfolio_neg == 1)
  group_by(date, portfolio_combined) %>%
  summarize(
    ret = weighted.mean(W_RETURN, MKT_CAP),
    portfolio_pos = unique(portfolio_pos),
    .groups = "drop"
  )

value_portfolios %>% 
  group_by(portfolio_combined) %>%
  summarise(ret = mean(ret))

# 
# value_premium <- value_portfolios %>%
#   group_by(date, portfolio_pos) %>%
#   summarize(ret = mean(ret), .groups = "drop_last") %>%
#   summarize(value_premium = ret[portfolio_pos == max(portfolio_pos)] -
#     ret[portfolio_pos == min(portfolio_pos)])
# 
# mean(value_premium$value_premium * 100)
  
```

################
Short term abnormal return after an individual event
################
```{r}
data_weekly <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/all_data_weekly.csv", sep = ",") %>%
  select(date,isin,W_RETURN,PX_LAST,MKT_CAP,negative_sum,positive_sum) %>% 
  mutate(date = as.Date(date)) %>%
  left_join(stoxx600_index %>% select(date, mkt_excess), by = "date")

short_term_data_nested <- data_weekly %>% ungroup() %>%
  select(date,isin,W_RETURN,mkt_excess) %>%
  nest(data = -c(isin)) %>%
  mutate(beta = future_map(
    data, ~ roll_capm_estimation(.,weeks = 10, min_obs = 10)
  )) %>%
  unnest(c(beta)) %>%
  na.omit(beta,alpha) %>% select(-data)

short_term_data <- short_term_data_nested %>% 
  left_join(data_weekly, by = c("date","isin")) %>%
  mutate(
    pred_ret = alpha + beta*mkt_excess,
    abnormal_ret = W_RETURN - pred_ret) %>%
  select(-c(beta,alpha,pred_ret))

# Create columns that represent the lagged and leading values in relation to the event date:
for (i in 1:10) {
  
  short_term_data[paste0("-",i)] <- short_term_data %>% group_by(isin) %>% transmute(t = lag(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
  
  short_term_data[paste0("+",i)] <- short_term_data %>% group_by(isin) %>% transmute(t = lead(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
}


short_term_data_event = short_term_data %>%
  group_by(isin) %>%
  mutate(
    pos_mean = na_if(positive_sum, 0),
    positive_mean = mean(pos_mean,na.rm = TRUE)) %>%
  group_by(date,isin) %>%
  # Make sure that we only calculate the cutoff value of periods with actual observations. 
  mutate(event = 
    case_when(positive_sum > positive_mean ~ 1, TRUE
               ~ 0)
  ) %>%
  filter(event == 1) %>%
  ungroup() %>% 
  rename("0" = abnormal_ret) %>% 
  # Remove columns we don't use
  select(-c("W_RETURN","PX_LAST","mkt_excess","positive_sum","MKT_CAP","negative_sum","event","positive_mean","pos_mean")) %>%
  pivot_longer(!c(date,isin), names_to = "type", values_to = "value") %>% 
  na.omit(value) %>%
  group_by(isin,type) %>% 
  summarise(abnormal_ret = mean(value))


lagged_index = short_term_data_event %>%
  mutate(type = as.character(type),
         type = as.integer(type)) %>% filter(type <= 0) %>% 
  mutate(type = as.factor(type)) 

lagged_index$type <- factor(lagged_index$type, levels = c("0", "-1", "-2","-3","-4","-5","-6","-7","-8","-9","-10"))

lagged_index = lagged_index %>%
  arrange(isin,type) %>%
  group_by(isin) %>%
  mutate(index_ret = case_when(type == '0' ~ 0,
                        TRUE ~ abnormal_ret),
         index =  100*(cumprod(1+index_ret)))

lead_index = short_term_data_event %>%
  mutate(type = as.character(type),
         type = as.integer(type)) %>% filter(type > 0) %>% 
  mutate(type = as.factor(type)) 

lead_index$type <- factor(lead_index$type, levels = c("0", "1", "2","3","4","5","6","7","8","9","10"))

lead_index = lead_index %>%
  arrange(isin,type) %>%
  group_by(isin) %>%
  mutate(index_ret = case_when(type == '0' ~ 0,
                        TRUE ~ abnormal_ret),
         index =  100*(cumprod(1+index_ret)))

short_term_indexed = lead_index %>%
  rbind(lagged_index)

short_term_indexed$type <- factor(short_term_indexed$type, levels = c("-10", "-9","-8","-7","-6","-5","-4","-3","-2","-1","0", "1", "2","3","4","5","6","7","8","9","10"))

short_term_indexed %>% 
  ggplot(aes(x = type, y = index, colour = isin, group = isin)) + 
  geom_line() +
  theme(legend.position = "none")


short_term_indexed %>%
  group_by(type) %>%
  summarise(abnormal_ret = mean(abnormal_ret)) %>% 
  ggplot(aes(x = type, y = abnormal_ret)) +
  geom_point() +
  theme(legend.position = "none")
```
