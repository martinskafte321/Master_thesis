\subsection{The Event Study Methodology}

On the surface, measuring the effect of an economic event is often a difficult task for economists. However, with frequent and efficient data from the financial markets, constructing an event study allows a researcher to measure the impact of an isolated event on the value of a firm.  

The methodology is used to measure the market response to corporate events. The foundation of event studies emerge from the \textit{efficient market hypothesis} \citep{fama1969_EMH}, which states that security prices reflect all present information. Given rationality on the markets, the impact of an event will be reflected immediately in the price of a security.  Within finance research, event studies has been applied to measure market reactions to firm specific events such as mergers and acquisitions and earnings announcements. Various event study methodologies has been discussed in the literature, however the baseline of my structure is centered on \citep{Event_studies}. The methodology draws on specific assumptions including semi-strong market efficiency, unanticipated events, and non-confounding events. \cite{sorescu2017event} examines the issue of confounding events
and do not find it problematic in short term event studies. For this reason, I do not eliminate overlapping events across different firms in the analysis. However, I do remove overlapping dates for individual firms in order to avoid measuring a return more than once.

The initial task of establishing an event study is to outline the event of interest and determine the window over which the security prices will be inspected. The latter is dependent on the former, as various events require different examination time. New information related to public news, charges, and accusations may require wide windows to reflect all information. To investigate the full impact of events, I apply event windows of 21 days around an event for the short term inspection and portfolio holding periods of 1-12 months for the long term. 

Furthermore, the examination requires a measure of impact on firm performance from an event. \cite{Event_studies} outlines the appropriate measure through the "abnormal return" associated with an event, defined as the difference between the realized ex-post return and the expected return. The latter is characterized as the expected return conditional on the event not taking place, as:  
\begin{equation}
    AR_{i,t} = R_{i,t} - E[R_{i,t} \mid X_t ]
\end{equation}
where $AR_{i,t}$ is the abnormal return as the difference between the realized return, $R_{i,t}$, and the expected return, $E[R_{i,t} \mid X_t ]$, for firm \textit{i} at time \textit{t}. $X_t$ is the conditional information from an expected returns model. 

\subsection{The Market Model}



To assess the short term impact on corporate events, the examination covers 21 trading days surrounding an event. This window spans 10 days prior to the event, the actual event day, and 10 days following it. I include 10 days before to detect possible leakages or reactions before a spike in news articles occur. Additionally, the subsequent 10-day period helps determine if the event itself has a notable impact. To quantify the impact through abnormal returns, it is necessary to establish a model for expected returns. 

Several models have been applied in the literature to estimate the short-term abnormal returns in event studies, including the CAPM, the Market Model, and various multi-factor models. According to \cite{holler2014event} the most widely used is the Market Model, which assumes a linear relationship between the individual stock return and the market return.  The Market Model considers a single factor by linking the return of a specific security to the return of a market portfolio. Since I am analyzing the relation on European stocks, I assume the market portfolio to be the STOXX Europe 600. In this model, I assume that asset returns follow a jointly multivariate normal distribution and are independently and identically distributed over time. For a particular stock, the model is defined as follows:

\begin{equation} \label{market_model}
    R_{i,t} = \alpha_i + \beta_i * R_{m,t} + \epsilon_{i,t},
\end{equation}
 where $R_{i,t}$ is the return of the stock \textit{i} on day \textit{t}, $\alpha_i$ is the regression intercept\footnote{The excess return of a stock that is not explained by its sensitivity to the market return}, $R_{m,t}$ is the market return on day, and
 $\beta_i$ is the sensitivity of $R_{i,t}$ to the returns of a market portfolio.
 
The estimation window of the regression is set to 120 days prior to the event, as proposed by \cite{Event_studies}, to mitigate for sampling error and serial correlation in abnormal returns. In practical terms, this corresponds to the time frame of $[-131;-11]$ before the event, excluding the days potentially affected by the event to prevent contamination. Under the null hypothesis that the event does not affect returns, the distributional properties of the abnormal returns can be used to draw inferences in the event period. 

I apply the linear ordinary least squares (OLS) in the estimation of expected returns. \cite{brown1985using} investigate the properties of applying daily stock returns in event studies. They find that the methodologies based on the OLS market model are well-specified under a variety of conditions. It is common knowledge that daily returns may suffer from non-normality. However the mean excess return in a cross-section of securities converges to normality as the amount of sample securities increase. For hypothesis tests over multi-day intervals, autocorrelation plays a minor role. An absence of clustering in events is sufficient for the requirement of a consistent variance estimator and for deeming cross-correlation irrelevant, as the abnormal returns will be independent across securities.  With the expected returns predicted from the Market Model, the abnormal returns (AR) are defined as the residual between the realized and the expected returns for each event:
 \begin{equation}
    AR_{i,t} = R_{i,t} - (\hat{\alpha_i} + \hat{\beta_i} * R_{m,t}).
 \end{equation} 
$AR_{i,t}$ measures the shareholders' reaction to the event of firm \textit{i} at time \textit{t}. The individual abnormal returns can be aggregated as the average abnormal return (AAR) on a given day, by:  
 \begin{equation}
 AAR_t = \frac{1}{N} \sum_{i=1} ^N AR_{i,t},
 \end{equation}
 \begin{equation}
 var(AAR_{t}) = \frac{1}{N^2} \sum_{i=1} ^N \sigma_{\epsilon_i} ^2.
 \end{equation}
I aggregate the AAR from $t_1$ to $t_2$ to accommodate a multiple period event window as the cumulative AAR (CAAR), by:
 \begin{equation}
 CAAR_{[t_1,t_2]} = \sum_{t = t_1 } ^{t_2} AAR_{t},
 \end{equation}
 \begin{equation}
 var(CAAR_{[t_1,t_2]}) = \sum ^{t_2}_{t=t_1} var(AAR_t).
 \end{equation}

To test the null hypothesis that the abnormal returns are zero, I can draw inferences about the CAAR by the distribution: $CAAR_{[t_1,t_2]} \sim N[0,var(CAAR_{[t_1,t_2]})]$. In this analysis, I utilize the sample variance derived from the market model regression as an estimator of $\sigma^2_{\epsilon_i}$. The distribution is asymptotic with the the number of stocks N and the length of the estimation window \citep{Event_studies}.  
 
 

\subsection{Long-term abnormal returns}

When transitioning from short-term measurement horizons to long-term ones, there are two crucial considerations to account for when estimating risk-adjusted returns. These considerations involve appropriately adjusting for risk and establishing a model for expected returns. First, in long-term tests, even minor inaccuracies in risk adjustment can lead to substantial disparities in measured abnormal performance. i.e. due to unusual prior performance. Consequently, in long-term event studies, abnormal performance is measured based on post-event risk estimates rather than historical ones. Second, as the objective of event studies is to isolate the event and measure the corresponding abnormal returns, it is necessary to employ an expected return model that effectively distinguishes performance from other known determinants of performance \cite{kothari}. After isolating and measuring the event-related abnormal returns, the researcher need to test the null hypothesis that the distribution of the long horizon abnormal returns concentrates around zero.   

In an evaluation of long-horizon event study methodologies, \cite{Ang_event_method} highlight two approaches which have been utilized substantially in the finance literature. The Buy-and-Hold Approach calculates the abnormal return as the difference between the return from the event firm and a benchmark, and tests the null hypothesis that the abnormal return is zero. The Calendar-Time Portfolio (CTP) approach forms a portfolio each month consisting of firms that have experienced an event prior to the month, and tests the null hypothesis that the intercept is zero in a regression against the factors of an asset-pricing model.

The measurement of long horizon abnormal returns are challenged by misspecification of test statistics. \cite{Lyon_1997_test_stats} studies the empirical power in long-term abnormal stock returns and identifies three sources for misspecification: New listing bias\footnote{The new listing bias arises since sample firms in event studies often have a long posterior history of returns, while the constituents of a reference portfolio may include firms that began trading subsequent to the event period.}, rebalancing bias\footnote{The rebalancing bias emerge since the compound returns of a reference portfolio, such as a weighed market index, are typically calculated assuming periodic rebalancing, while the returns of sample firms are compounded without rebalancing. }, and skewness bias\footnote{ the skewness bias arises because long-run returns are typically positively skewed.}. Whether these biases give rise to misspecification depends on the method used in identifying abnormal returns. 

The Buy-and-Hold approach can mitigate the new listing and rebalancing bias by applying a benchmark consisting of a single firm with similar size and book-to-market. However, to eliminate the skewness bias, one needs to either 1) run a bootstrapped version of a skewness-adjusted t-statistic, or 2) find empirical p-values calculated from a simulated distribution of mean long-run abnormal returns from pseudo-portfolios. However, the approach might also suffer from cross-correlation bias, which arises because matching on firm characteristics may fail to completely remove the correlation between firm's returns. To circumvent the aforementioned biases and the challenges associated with identifying benchmark firms for the complete sample of 600 firms, as well as the complexities of reproducing the research findings, I disregard the Buy-and-Hold approach and employ the CTP model. While the CTP approach effectively addresses the biases mentioned earlier, it may encounter challenges related to selecting an appropriate asset pricing model and dealing with heteroskedasticity in portfolio returns \cite{lyon1999improved}.

\subsubsection{Calendar Time Portfolio}

To implement the approach, a portfolio is constructed each calendar month consisting of all firms experiencing an event within the preceding \textit{T} months. The monthly portfolio return is computed as the weighted average return of firms that experienced an event within the preceding T months:
\begin{equation}
    R_{p,t} = \sum_{i=1} ^{N} \frac{w_{i,t} R_{i,t} }{ w_{i,t}  } .
\end{equation}
Where $R_{i,t}$ is the stock \textit{i} return in period \textit{t} and \textit{N} is the amount of stocks in the portfolio in period \textit{t}. 
The portfolio excess returns are regressed against the factors in the \cite{fama2015five} five-factor model to determine whether the portfolio has achieved abnormal returns after accounting for the returns of the factors. The model aims to describe the risk of the general market through the following; 1) the market risk premium, and the monthly portfolio returns of the following common factors: 2) small minus big market cap (SMB), 3) high minus low book-to-market (HML), 4) robust minus weak profitability (RMW), and 5) conservative minus aggressive investments (CMA). The representation of the Fama-French five-factor model is: 
\begin{equation} \label{eq: FF5}
    R_{p,t} - R_{rf,t} = \alpha_p + \beta_1(R_{m,t} - R_{rf,t}) + \beta_2 SMB_t + \beta_3 HML_t + \beta_4 RMW_t + \beta_5 CMA_t \epsilon_{p,t} 
\end{equation}
with $R_{rf,t}$ being the risk-free rate\footnote{US treasuries / EUR treasuries} at time \textit{t} and the $\beta$'s representing the sensitivity of the portfolio return towards the risk factors. Under the assumption that the five-factor model adequately explains the variability in expected stock returns, the regression intercept, denoted as $\alpha$, measures the risk-adjusted average abnormal performance of event firms in the sample. In the absence of abnormal returns, this intercept should ideally be zero. Therefore, the accuracy of the estimation relies on the model's capacity to explain the risk factors prevalent in the market. I make inferences using a t-statistic derived from the time-series of monthly calendar-time abnormal returns. If the test results indicate that the $\alpha$ is not statistically different from zero, the time-series aligns with the asset pricing model, which implies that the event had no significant long-term effect. Conversely, if the test reveals significant long-term abnormal returns, through a significant $\alpha$, it suggests that the event had an impact.

\cite{fama1998_events} outlines the advantages of the CTP approach as emerging from the use of monthly returns and rebalancing. He argues that monthly returns are less skewed, which makes the model less sensitive to the bad model problem of describing average returns. Further, the time-series variation of the monthly rebalancing captures the effect of the cross-correlation between stock returns. Since the number of stocks in the portfolio varies from one month to another, the error term of the regression may be heteroskedastic. For example, \cite{ritter1995} argues that anomalies can be understated if events cluster in time due to, e.g., timing of events. However, I alleviate this issue by using weighted least squares (WLS) in the regression with the monthly amount of firms as weights, as suggested by \cite{Ang_event_method}. In addition, as homoskedastic residuals are often a harsh assumption, I compute heteroskedastic-robust standard errors from the approach of \cite{white1980heteroskedasticity}. Overall, \cite{lyon1999improved} report that the calendar-time portfolio approach together with a Fama-French factor model is well specified for random samples in their simulation study. 

Portfolio returns can be computed through either value-weighted or equal-weighted approaches. \cite{fama1998_events} suggest that employing value-weighting for event firms can help mitigate apparent anomalies by better reflecting the wealth effect encountered by investors. Moreover, an equal-weighted portfolio would assign greater weight to small stocks, potentially amplifying the potentially amplifying the biases arising from challenges in asset-pricing models' ability to explain the average returns of such stocks. Consequently, the preferred approach is to apply value weights based on market capitalization when calculating portfolio returns both on short and long horizons. 

In academic literature, the models for short- and long term expected returns and the event study methodology have been extensively employed to evaluate the impact of specific events. By leveraging these well-established techniques and aiming to uncover further insights, I will incorporate an advanced approach to measure investor reactions in my research.

\subsection{Data and general setup}

The potential amount of events related to sustainable development is enormous, with a multitude of news articles being published worldwide on a daily basis. To comprehensively evaluate the underlying drivers behind the financial impacts and their implications for a diverse portfolio, we need a database with historical records and frequent observations. When conducting an event study, researchers commonly construct their databases by either collecting events from archives or utilizing existing databases that encompass a comprehensive and thoughtfully selected set of events. However, both approaches share a limitation in that they rely on databases comprising of hand-picked events, which undermine the practical use cases of these results. First, the manual ex-post definition of events introduces a potential bias in the selection of events for the sample. This bias arises because events that are included in the sample are typically chosen based on their desirable characteristics and outcomes from an ex-post perspective, while inadvertently excluding events that may have been significant from a shareholder's perspective, which eventually can skew the representation of events. Second, although short-term reactions to well-defined events are empirically evident, practitioners are unable to capitalize on these circumstances. The identification of events relies on information that, in certain situations, may not be available at the time of action. That is, while we can measure an event ex-post, we lack the ability to react to it in real-time. In other words, the retrospective nature of event identification prevents us from proactively responding to events as they unfold. \\

In this study I utilize a data set called SDG Signals provided by Matter\footnote{ https://www.thisismatter.com/sdg-signals}. The company has developed a systematic collection of daily news articles from more than 150.000 media sources. They measure the volume of which individual companies are mentioned in a positive, negative, or neutral context concerning the United Nations' 17 Sustainable Development Goals (SDGs) across global news articles. The database is updated at midnight each day and employs artificial intelligence techniques to associate individual news articles with more than 75,000 publicly traded companies. It assesses the relevance of each article to a specific SDG or in a broader context. 

Therefore, based on the observed volume of news articles related to a specific company on a particular day, I make the assumption that it reflects the severity of the events experienced by that company. This assumption allows me to utilize the daily news article volume as a proxy for the significance of events.

Accordingly, the specific events examined in this study are not explicitly defined within the dataset itself. Instead, I employ a rule-based approach to identify significant spikes in the volume of news articles related to individual companies. This short-term analysis is based on daily observations and identifies an event when the number of news articles surpasses a predetermined threshold. To determine the threshold, I use a one standard deviation move from the average daily article count for a given firm. Additionally, the threshold requires the amount of positive news articles to be at least double  that of negative news articles, and vice versa, on any given day. This criteria is implemented in order to avoid misleading events, as the amount of negative articles is generally lower than positive ones.Without this assumption, a non-negligible number of events would be identified in the sample, even in cases where the number of negative articles is considerably lower than the number of positive articles, which I would classify as non-events. News articles published on a Saturday or Sunday are attributed to the following Monday, as the share price response will manifest here. This approach carries the risk of artificially inflating the number of observed articles on Mondays. To mitigate this potential bias, I divide the count of articles on Mondays by three to level the effects. To ensure the accuracy of the analysis, I decontaminate the daily count of news articles for each company by subtracting the number of irrelevant articles from the total count. 

The short term methodology produces 1.618 negative events and 10.275 positive events from 401 unique companies from January 2018 to January 2023. Hence, approximately two-thirds of the sample firms experienced at least one event during this period. For the long-term analysis, I follow a similar approach but aggregate the daily news articles into monthly figures, upon which events are identified based on significant spikes in the monthly sum. Using this method, I identify a total of 1.117 negative events and 2.054 positive events among 437 distinct companies. The events are evenly distributed throughout the 5 year period, with the exception of the initial months of the COVID-19 pandemic, as illustrated in figure \ref{fig:event_distribution} in the appendix. The distribution is sufficient evidence to confirm that events are not clustering in calendar time. Moreover, figure \ref{fig:event_distribution_SDG} in the appendix shows the events are not evenly distributed across sustainability goals, as SDGs 3, 7, 9, 12, 13, 16, and 17 receive considerably more media attention than the remainders. The amount of events increases considerably when conditioning on the various SDGs, as this involves the possibility of several events per day. 

To investigate the validity of the hypotheses on the European financial markets, I select all the constituents of the \textit{STOXX Europe 600} index that were actively traded on or after January 1, 2018. This index represents approximately 90\% of the market capitalization of the developed European equity market. After excluding companies that did not experience any events on a daily or monthly basis, the dataset consists of a total of N = 437 stocks.  

Daily and monthly market capitalization\footnote{I use the free-float market capitalization calculated in Euro to best reflect the market movements} and returns, adjusted for stock splits and dividends, are collected from Bloomberg\footnote{https://www.bloomberg.com.}. Additionally, I extend the dataset to include ESG Risk Ratings provided by Sustainalytics\footnote{https://www.sustainalytics.com/esg-ratings. The ratings are the contemporaneous ratings and not a historical time series. As the analyzed period is only five years, I assume that the actual changes in ratings would not affect my results significantly.}. These ratings assess the level of ESG risk for individual firms, categorizing them as negligible, low, medium, high, or severe. Due to the limited number of firms in the negligible and severe groups, they are added to the low and high groups, respectively, to exploit the data fully. 

To examine the relationship between SDG-related news and corporate performance, I identify significant events using the methods described above on both a daily and monthly basis. I then calculate the corresponding abnormal returns to analyze the impact of these events. 