sdg == "sdg 16"  ~ "Peace",
sdg == "sdg 17" ~ "Partnership")
)
all = t %>%
mutate(period = as.numeric(period)) %>%
filter(period >= -10) %>%
filter(period <= 10) %>%
mutate(period = as.factor(period),
sdg = as.factor(sdg)) %>%
mutate(period = factor(period, levels = c("-10","-9","-8","-7","-6","-5","-4","-3","-2","-1","0", "1", "2","3","4","5","6","7","9","10"))) %>% na.omit() %>%
arrange(isin,sdg,date,period) %>%
group_by(isin,sdg,date) %>%
mutate(csum = cumsum(abnormal_ret))
all = t %>%
mutate(period = as.numeric(period)) %>%
filter(period >= -10) %>%
filter(period <= 10) %>%
mutate(period = as.factor(period),
sdg = as.factor(sdg)) %>%
mutate(period = factor(period, levels = c("-10","-9","-8","-7","-6","-5","-4","-3","-2","-1","0", "1", "2","3","4","5","6","7","9","10"))) %>% na.omit() %>%
arrange(isin,sdg,date,period) %>%
group_by(isin,sdg,date) %>%
mutate(csum = cumsum(abnormal_ret))
bands = all %>%
group_by(sdg,period) %>%
summarise(n = n(),
sd = sd(csum),
se = sd/sqrt(n),
t = 1.96,
CI=t*se
)
all %>%
group_by(sdg,period) %>%
summarise(CAAR = mean(csum),
AAR = weighted.mean(abnormal_ret,MKT_CAP),
n = n()) %>% ungroup() %>%
left_join(bands, by = c("sdg","period")) %>%
mutate(period = as.character(period)) %>%
filter(period == '10') %>%
mutate(sdg = factor(sdg, levels = c("People","Planet","Prosperity","Peace","Partnership"))) %>%
# mutate(sdg = factor(sdg, levels = c("sdg 1", "sdg 2","sdg 3","sdg 4","sdg 5","sdg 6","sdg 7","sdg 8","sdg 9","sdg 10","sdg 11",
#                                      "sdg 12", "sdg 13","sdg 14","sdg 15","sdg 16","sdg 17"))) %>%
ggplot(
aes(x = sdg, y = CAAR, color= sdg,group = sdg, fill = sdg)) +
geom_bar(stat="identity") +
geom_errorbar( aes(x=sdg, ymin=CAAR-CI, ymax=CAAR+CI), colour = "orange", width=0.5, alpha=0.8, size=0.7) +
scale_y_continuous(name ="Abnormal return",
labels = scales::percent) +
#labs(x = "Five Pillars of SDGs") +
theme_bw()+
theme(legend.title = element_blank(),
legend.key.size = unit(0.5, 'cm'),
axis.text.x = element_text(angle=45)) +
theme(legend.position = "none")
all %>%
group_by(sdg,period) %>%
summarise(CAAR = weighted.mean(csum,MKT_CAP),
AAR = weighted.mean(abnormal_ret,MKT_CAP),
n = n()) %>% ungroup() %>%
left_join(bands, by = c("sdg","period")) %>%
mutate(period = as.character(period)) %>%
filter(period == '10') %>%
mutate(sdg = factor(sdg, levels = c("People","Planet","Prosperity","Peace","Partnership"))) %>%
# mutate(sdg = factor(sdg, levels = c("sdg 1", "sdg 2","sdg 3","sdg 4","sdg 5","sdg 6","sdg 7","sdg 8","sdg 9","sdg 10","sdg 11",
#                                      "sdg 12", "sdg 13","sdg 14","sdg 15","sdg 16","sdg 17"))) %>%
ggplot(
aes(x = sdg, y = CAAR, color= sdg,group = sdg, fill = sdg)) +
geom_bar(stat="identity") +
geom_errorbar( aes(x=sdg, ymin=CAAR-CI, ymax=CAAR+CI), colour = "orange", width=0.5, alpha=0.8, size=0.7) +
scale_y_continuous(name ="Abnormal return",
labels = scales::percent) +
#labs(x = "Five Pillars of SDGs") +
theme_bw()+
theme(legend.title = element_blank(),
legend.key.size = unit(0.5, 'cm'),
axis.text.x = element_text(angle=45)) +
theme(legend.position = "none")
all %>%
group_by(sdg,period) %>%
summarise(CAAR = weighted.mean(csum,MKT_CAP),
AAR = weighted.mean(abnormal_ret,MKT_CAP),
n = n())
-6.388057e-03
-6.388057e-03*100
all
all %>% filter(period == '0') %>% filter(sdg == 'Peace') %>% group_by(date) %>% summarise(sum = n())
all %>% filter(period == '0') %>% filter(sdg == 'Peace') %>% group_by(date) %>% summarise(sum = n()) %>% ggplot(aes(x = date, y = n)) + geom_bar(stat = "identity")
all %>% filter(period == '0') %>% filter(sdg == 'Peace') %>% group_by(date) %>% summarise(sum = n()) %>% ggplot(aes(x = date, y = n)) + geom_bar()
all %>% filter(period == '0') %>% filter(sdg == 'Peace') %>% group_by(date) %>% summarise(sum = n())
all %>% filter(period == '0') %>% filter(sdg == 'Peace') %>% group_by(date) %>% summarise(sum = n()) %>% ggplot(aes(x = date, y = sum)) + geom_bar(stat = "identity")
all %>% filter(period == '0') %>% filter(sdg == 'Peace') %>% group_by(date) %>% summarise(sum = mean(abnormal_ret) %>% ggplot(aes(x = date, y = sum)) + geom_bar(stat = "identity")
all %>% filter(period == '0') %>% filter(sdg == 'Peace') %>% group_by(date) %>% summarise(sum = mean(abnormal_ret) %>% ggplot(aes(x = date, y = sum)) + geom_line()
all %>% filter(period == '0') %>% filter(sdg == 'Peace') %>% group_by(date) %>% summarise(sum = mean(abnormal_ret) %>% ggplot(aes(x = date, y = sum))
all %>% filter(period == '0') %>% filter(sdg == 'Peace') %>% group_by(date) %>% summarise(sum = mean(abnormal_ret)
all %>% filter(period == '0') %>% filter(sdg == 'Peace') %>% group_by(date) %>% summarise(sum = mean(abnormal_ret))  %>% ggplot(aes(x = date, y = sum))
all %>% filter(period == '0') %>% filter(sdg == 'Peace') %>% group_by(date) %>% summarise(sum = mean(abnormal_ret))
all %>% filter(period == '0') %>% filter(sdg == 'Peace') %>% group_by(date) %>% summarise(sum = mean(abnormal_ret))  %>% ggplot(aes(x = date, y = sum)) + geom_line()
all %>% filter(period == '0') %>% filter(sdg == 'Peace') %>% group_by(date) %>% summarise(sum = mean(csum))  %>% ggplot(aes(x = date, y = sum)) + geom_line()
all %>%
group_by(sdg,period) %>%
summarise(CAAR = weighted.mean(csum,MKT_CAP),
AAR = weighted.mean(abnormal_ret,MKT_CAP),
n = n())
short_term_data <- betas %>%
left_join(short_term_data_daily, by = c("date","isin")) %>%
group_by(isin) %>%
mutate(
pred_ret = alpha + beta*mkt_excess,
abnormal_ret = W_RETURN - pred_ret,
norm_positive_sum = sentiment_positive_count - sdg_not_relevant_positive_count,
norm_negative_sum = sentiment_negative_count - sdg_not_relevant_negative_count
) %>%
select(-c(beta,alpha,pred_ret,sdg_not_relevant_positive_count,sdg_not_relevant_negative_count))
for (i in 1:10) {
short_term_data[paste0("-",i)] <- short_term_data %>% group_by(isin) %>% transmute(t = lag(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
short_term_data[paste0("+",i)] <- short_term_data %>% group_by(isin) %>% transmute(t = lead(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
short_term_data[paste0("neg_event_-",i)] <- short_term_data %>% group_by(isin) %>% transmute(l = lag(norm_negative_sum,i)) %>% ungroup() %>% select(-isin)
short_term_data[paste0("neg_event_+",i)] <- short_term_data %>% group_by(isin) %>% transmute(l = lead(norm_negative_sum,i)) %>% ungroup() %>% select(-isin)
short_term_data[paste0("pos_event_-",i)] <- short_term_data %>% group_by(isin) %>% transmute(l = lag(norm_positive_sum,i)) %>% ungroup() %>% select(-isin)
short_term_data[paste0("pos_event_+",i)] <- short_term_data %>% group_by(isin) %>% transmute(l = lead(norm_positive_sum,i)) %>% ungroup() %>% select(-isin)
}
for (i in 1:10) {
short_term_data[paste0("-",i)] <- short_term_data %>% group_by(isin) %>% transmute(t = lag(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
short_term_data[paste0("+",i)] <- short_term_data %>% group_by(isin) %>% transmute(t = lead(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
short_term_data[paste0("neg_event_-",i)] <- short_term_data %>% group_by(isin) %>% transmute(l = lag(norm_negative_sum,i)) %>% ungroup() %>% select(-isin)
short_term_data[paste0("neg_event_+",i)] <- short_term_data %>% group_by(isin) %>% transmute(l = lead(norm_negative_sum,i)) %>% ungroup() %>% select(-isin)
short_term_data[paste0("pos_event_-",i)] <- short_term_data %>% group_by(isin) %>% transmute(l = lag(norm_positive_sum,i)) %>% ungroup() %>% select(-isin)
short_term_data[paste0("pos_event_+",i)] <- short_term_data %>% group_by(isin) %>% transmute(l = lead(norm_positive_sum,i)) %>% ungroup() %>% select(-isin)
}
short_term_data_event = short_term_data %>%
group_by(isin) %>%
mutate(
norm_positive_sum = na_if(norm_positive_sum, 0),
norm_negative_sum = na_if(norm_negative_sum, 0),
positive_threshold = mean(norm_positive_sum,na.rm = TRUE) + 1*sd(norm_positive_sum,na.rm = TRUE),
negative_threshold = mean(norm_negative_sum,na.rm = TRUE) + 1*sd(norm_negative_sum,na.rm = TRUE)
) %>%
rename("0" = abnormal_ret) %>%
select(-c("W_RETURN","mkt_excess",
#"positive_threshold","negative_threshold","norm_positive_sum","norm_negative_sum",
"sentiment_negative_count","sentiment_positive_count",
#"roll_mean_norm_negative_sum","roll_mean_norm_positive_sum", "roll_sum_norm_negative_sum","roll_sum_norm_positive_sum"
)) %>%
group_by(date,isin) %>%
# Make sure that we only calculate the cutoff value of periods with actual observations.
mutate(pos_event =
case_when(norm_positive_sum > positive_threshold & norm_positive_sum > 10 & norm_positive_sum > 2*norm_negative_sum ~ 1,
TRUE ~ 0),
neg_event =
case_when(norm_negative_sum > negative_threshold & norm_negative_sum > 10 & norm_negative_sum > 2*norm_positive_sum ~ 1,
TRUE ~ 0)
)
negative_index = short_term_data_event %>%
select(date,isin,MKT_CAP,"0",contains("-"),contains("+"),neg_event) %>%
select(-c(contains("pos_event_"),contains("neg_event_"))) %>%
filter(neg_event == 1) %>%
select(-neg_event) %>%
pivot_longer(!c(date,isin,MKT_CAP), names_to = "type", values_to = "value") %>%
na.omit(value)
negative_index
negative_index %>% filter(type == '0')
negative_index %>% filter(type == '0') %>% group_by(date) %>% summarise(sum = sum(value))
negative_index %>% filter(type == '0') %>% group_by(date) %>% summarise(sum = sum(value)) %>% ggplot(aes(x = date, y = sum)) + geom_line()
negative_index %>% filter(type == '0') %>% group_by(date) %>% summarise(sum = mean(value)) %>% ggplot(aes(x = date, y = sum)) + geom_line()
negative_index %>% filter(type == '0') %>% filter(date >= '2019-02-01')
negative_index %>% filter(type == '0') %>% filter(date >= '2019-02-01') %>% filter(date <= '2019-06-01')
negative_index %>% filter(type == '0') %>% filter(date >= '2019-02-01') %>% filter(date <= '2019-06-01') %>% group_by(date) %>% summarise(m = mean(value))
negative_index %>% filter(type == '0') %>% filter(date >= '2019-02-01') %>% filter(date <= '2019-06-01') %>% group_by(date) %>% summarise(m = sum(value))
negative_index %>% filter(type == '0') %>% filter(date == '2019-04-10')
negative_index %>% filter(type == '0') %>% group_by(date) %>% summarise(sum = mean(value)) %>% ggplot(aes(x = date, y = sum)) + geom_line()
negative_index %>% filter(type == '0') %>% group_by(date) %>% summarise(sum = weighted.mean(value,MKT_CAP)) %>% ggplot(aes(x = date, y = sum)) + geom_line()
negative_index %>%  left_join(ESG_RR, by = "isin") %>% filter(ESG_risk_category == 'Low') %>% filter(type == '0') %>% group_by(date) %>% summarise(sum = weighted.mean(value,MKT_CAP)) %>% ggplot(aes(x = date, y = sum)) + geom_line()
negative_index %>%  left_join(ESG_RR, by = "isin") %>% filter(ESG_risk_category == 'Medium') %>% filter(type == '0') %>% group_by(date) %>% summarise(sum = weighted.mean(value,MKT_CAP)) %>% ggplot(aes(x = date, y = sum)) + geom_line()
negative_index %>%  left_join(ESG_RR, by = "isin") %>% filter(ESG_risk_category == 'High') %>% filter(type == '0') %>% group_by(date) %>% summarise(sum = weighted.mean(value,MKT_CAP)) %>% ggplot(aes(x = date, y = sum)) + geom_line()
negative_index = short_term_data_event %>%
select(date,isin,MKT_CAP,"0",contains("-"),contains("+"),neg_event) %>%
select(-c(contains("pos_event_"),contains("neg_event_"))) %>%
filter(neg_event == 1) %>%
select(-neg_event) %>%
pivot_longer(!c(date,isin,MKT_CAP), names_to = "type", values_to = "value") %>%
na.omit(value)
negative_index_events_mean = short_term_data_event %>%
select(date,isin,neg_event,norm_negative_sum, contains("neg_event")) %>%
filter(neg_event == 1) %>%
rename('0' = norm_negative_sum) %>%
select(-neg_event) %>%
pivot_longer(!c(date,isin), names_to = "type", values_to = "amount") %>%
mutate_at("type", str_replace, "neg_event_", "") %>%
group_by(isin, date)
negative_index_events = negative_index_events_mean %>%
left_join(negative_index_events_mean %>% filter(type == '0') %>% select(isin,date,"mean" = amount), by = c("isin","date")) %>%
mutate(amount = ifelse(is.na(amount), 0, amount)) %>%
mutate(relative_events = amount/mean)
negative_index_all = negative_index %>%
left_join(negative_index_events %>% select(isin,date,type,relative_events), by = c("isin","date","type")) %>%
mutate(type = factor(type, levels = c("-10", "-9","-8","-7","-6","-5","-4","-3","-2","-1",
"0", "+1", "+2","+3","+4","+5","+6","+7","+8","+9","+10"))) %>%
arrange(isin,date,type) %>%
group_by(isin,date)
negative_index %>% filter(type == '0') %>% filter(date == '2019-04-10')
negative_index_10 = negative_index_all %>%
left_join(ESG_RR, by = "isin") %>%
mutate(csum = cumsum(value)) %>%
group_by(type,ESG_risk_category) %>% na.omit() %>%
summarise(
rel_events = mean(relative_events, na.rm = TRUE),
AAR = weighted.mean(value,MKT_CAP),
n = n(),
CAAR = weighted.mean(csum,MKT_CAP),
SE_AAR = sd(value, na.rm = TRUE)/sqrt(n()),
SE_CAAR = sd(csum, na.rm = TRUE)/sqrt(n())
)
negative = negative_index_10 %>%
transmute("Relative # events" = rel_events/100, type, ESG_risk_category
,AAR,CAAR) %>%
pivot_longer(!c(type,ESG_risk_category), names_to = "metric", values_to = "ret") %>%
left_join(
negative_index_10 %>% select(type,ESG_risk_category, "AAR" = SE_AAR, "CAAR" = SE_CAAR) %>%
pivot_longer(!c(type,ESG_risk_category), names_to = "metric", values_to = "SE"), by = c("metric","type","ESG_risk_category")
) %>%
mutate(
low = ret - 1.96*SE,
high = ret + 1.96*SE
)
negative %>% filter(metric == 'CAAR') %>% mutate(ESG_risk_category = factor(ESG_risk_category, levels = c("Low", "Medium","High"))) %>%
group_by(ESG_risk_category) %>%
ggplot(aes(x = type, y = ret,colour = ESG_risk_category, group = ESG_risk_category)) +
geom_ribbon(aes(ymin = low, ymax = high, fill = ESG_risk_category),
alpha=0.1, linetype="dashed", show.legend = FALSE) +
geom_line(size=1) +
theme_bw() +
theme(legend.title = element_blank(),
legend.position="top",
legend.direction = "horizontal") +
scale_y_continuous(name ="Abnormal return",
breaks = pretty_breaks(),
labels = scales::percent) +
scale_x_discrete(name = "Relative time", breaks = c("-10","-5","0","+5","+10"))
negative %>% filter(metric == 'CAAR') %>% mutate(ESG_risk_category = factor(ESG_risk_category, levels = c("Low", "Medium","High"))) %>% filter(isin != 'GB00BN4HT335')
negative %>% filter(metric == 'CAAR') %>% mutate(ESG_risk_category = factor(ESG_risk_category, levels = c("Low", "Medium","High"))) %>% filter(!isin %in% 'GB00BN4HT335')
negative %>% filter(metric == 'CAAR') %>% mutate(ESG_risk_category = factor(ESG_risk_category, levels = c("Low", "Medium","High")))
negative %>% filter(metric == 'CAAR') %>% mutate(ESG_risk_category = factor(ESG_risk_category, levels = c("Low", "Medium","High"))) %>% filter(!isin %in% 'GB00BN4HT335')
negative %>% filter(metric == 'CAAR') %>% mutate(ESG_risk_category = factor(ESG_risk_category, levels = c("Low", "Medium","High"))) %>% filter(!ISIN %in% 'GB00BN4HT335')
negative
negative_index_10 = negative_index_all %>% filter(!ISIN %in% 'GB00BN4HT335') %>%
left_join(ESG_RR, by = "isin") %>%
mutate(csum = cumsum(value)) %>%
group_by(type,ESG_risk_category) %>% na.omit() %>%
summarise(
rel_events = mean(relative_events, na.rm = TRUE),
AAR = weighted.mean(value,MKT_CAP),
n = n(),
CAAR = weighted.mean(csum,MKT_CAP),
SE_AAR = sd(value, na.rm = TRUE)/sqrt(n()),
SE_CAAR = sd(csum, na.rm = TRUE)/sqrt(n())
)
negative_index_10 = negative_index_all %>% filter(!isin %in% 'GB00BN4HT335') %>%
left_join(ESG_RR, by = "isin") %>%
mutate(csum = cumsum(value)) %>%
group_by(type,ESG_risk_category) %>% na.omit() %>%
summarise(
rel_events = mean(relative_events, na.rm = TRUE),
AAR = weighted.mean(value,MKT_CAP),
n = n(),
CAAR = weighted.mean(csum,MKT_CAP),
SE_AAR = sd(value, na.rm = TRUE)/sqrt(n()),
SE_CAAR = sd(csum, na.rm = TRUE)/sqrt(n())
)
negative_index_10
negative = negative_index_10 %>%
transmute("Relative # events" = rel_events/100, type, ESG_risk_category
,AAR,CAAR) %>%
pivot_longer(!c(type,ESG_risk_category), names_to = "metric", values_to = "ret") %>%
left_join(
negative_index_10 %>% select(type,ESG_risk_category, "AAR" = SE_AAR, "CAAR" = SE_CAAR) %>%
pivot_longer(!c(type,ESG_risk_category), names_to = "metric", values_to = "SE"), by = c("metric","type","ESG_risk_category")
) %>%
mutate(
low = ret - 1.96*SE,
high = ret + 1.96*SE
)
negative %>% filter(metric == 'CAAR') %>% mutate(ESG_risk_category = factor(ESG_risk_category, levels = c("Low", "Medium","High"))) %>%
group_by(ESG_risk_category) %>%
ggplot(aes(x = type, y = ret,colour = ESG_risk_category, group = ESG_risk_category)) +
geom_ribbon(aes(ymin = low, ymax = high, fill = ESG_risk_category),
alpha=0.1, linetype="dashed", show.legend = FALSE) +
geom_line(size=1) +
theme_bw() +
theme(legend.title = element_blank(),
legend.position="top",
legend.direction = "horizontal") +
scale_y_continuous(name ="Abnormal return",
breaks = pretty_breaks(),
labels = scales::percent) +
scale_x_discrete(name = "Relative time", breaks = c("-10","-5","0","+5","+10"))
positive_index %>% filter(type == '0') %>% filter(date == '2019-04-10') # GB00BN4HT335
# Positive
positive_index = short_term_data_event %>%
select(date,isin,MKT_CAP,"0",contains("-"),contains("+"),pos_event) %>%
select(-c(contains("pos_event_"),contains("neg_event_"))) %>%
filter(pos_event == 1) %>%
select(-pos_event) %>%
pivot_longer(!c(date,isin,MKT_CAP), names_to = "type", values_to = "value") %>%
na.omit(value)
positive_index %>% filter(type == '0') %>% filter(date == '2019-04-10') # GB00BN4HT335
positive_index %>% filter(type == '0') %>% group_by(date) %>% summarise(sum = mean(value)) %>% ggplot(aes(x = date, y = sum)) + geom_line()
knitr::opts_chunk$set(echo = TRUE)
# Load packages:
library(tidyverse)
library(lubridate)
library(readxl)
library(lmtest)
library(timetk)
library(dplyr)
library(zoo)
library(furrr)
library(ggplot2)
library(ggpubr)
library(xtable)
library(purrr)
library(stringr)
library(magrittr)
library(BSDA)
library(scales)
library(generics)
source("functions.R")
# Data connection to get STOXX 600 Europe sentiment and stock data. Joined on stock data, so there will be NA's in the sentiment.
# Sentiment and stock data merged together with weekly observations
data_monthly <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/all_data_month.csv", sep = ",") %>%
mutate(date = as.Date(date)) %>% select(date,isin, MKT_CAP,free_float_mkt_cap,W_RETURN, sentiment_negative_count,sentiment_positive_count,sdg_not_relevant_negative_count, sdg_not_relevant_positive_count)
# In the Fama French dataset, the variable "mkt_excess_ret" is created from the Bloomberg data as a market cap-weighted index.
eu_3factors_monthly = read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/eu_3factors_monthly.csv", sep = ",") %>%
na.omit() %>% select(-X,-w_ret) %>%
mutate(date = as.Date(date)) %>%
filter(date >= '2018-01-01')
eu_5factors_monthly = read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/eu_5factors_monthly.csv", sep = ",") %>%
na.omit() %>% select(-X) %>%
mutate(date = as.Date(date)) %>%
filter(date >= '2018-01-01')
rolling_means = list(1)
SDs = list(1)
emptylist_neg_1 = list()
emptylist_neg_2 = list()
y=1
z=1
long_term_data_monthly <- data_monthly %>%
left_join(ESG_RR, by ="isin") %>% filter(ESG_risk_category == 'High') %>%
transmute(date,isin,MKT_CAP, free_float_mkt_cap,
ret = W_RETURN,
norm_positive_sum = sentiment_positive_count - sdg_not_relevant_positive_count,
norm_negative_sum = sentiment_negative_count - sdg_not_relevant_negative_count,
) %>%
group_by(isin) %>%
transmute(date,isin,ret,MKT_CAP,free_float_mkt_cap,
norm_positive_sum = replace_na(norm_positive_sum,0),
norm_negative_sum = replace_na(norm_negative_sum,0),
roll_mean = across(!c(date,ret,MKT_CAP,free_float_mkt_cap),
~ rollmean(.x, k = y, fill = NA, align = "right"))) %>%
unnest(roll_mean,names_sep = "_") %>% arrange(isin) %>%
group_by(isin) %>%
mutate(
roll_mean_positive = na_if(roll_mean_norm_positive_sum, 0),
roll_mean_negative = na_if(roll_mean_norm_negative_sum, 0),
positive_threshold = mean(roll_mean_positive,na.rm = TRUE) + z*sd(roll_mean_positive,na.rm = TRUE),
negative_threshold = mean(roll_mean_negative,na.rm = TRUE) + z*sd(roll_mean_negative,na.rm = TRUE)
) %>%
group_by(date,isin) %>%
# Make sure that we only calculate the cutoff value of periods with actual observations.
mutate(
neg_event =
case_when(roll_mean_negative > negative_threshold & roll_mean_negative > 5 #& roll_mean_negative > 0.5*roll_mean_norm_positive_sum
~ 1,
TRUE ~ 0)
)
ESG_RR <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/ESG_ratings.csv") %>%
select("isin" = ISIN,ESG_risk_category) %>%
# mutate(ESG_risk_category =
#          case_when(
#   ESG_risk_category == "Severe" ~ "High",
#   ESG_risk_category == "Negligible" ~ "Low",
#   TRUE ~ ESG_risk_category)) %>%
filter(!ESG_risk_category %in% c('No data','Severe','Negligible')) %>% na.omit()
long_term_data_monthly <- data_monthly %>%
left_join(ESG_RR, by ="isin") %>% filter(ESG_risk_category == 'High') %>%
transmute(date,isin,MKT_CAP, free_float_mkt_cap,
ret = W_RETURN,
norm_positive_sum = sentiment_positive_count - sdg_not_relevant_positive_count,
norm_negative_sum = sentiment_negative_count - sdg_not_relevant_negative_count,
) %>%
group_by(isin) %>%
transmute(date,isin,ret,MKT_CAP,free_float_mkt_cap,
norm_positive_sum = replace_na(norm_positive_sum,0),
norm_negative_sum = replace_na(norm_negative_sum,0),
roll_mean = across(!c(date,ret,MKT_CAP,free_float_mkt_cap),
~ rollmean(.x, k = y, fill = NA, align = "right"))) %>%
unnest(roll_mean,names_sep = "_") %>% arrange(isin) %>%
group_by(isin) %>%
mutate(
roll_mean_positive = na_if(roll_mean_norm_positive_sum, 0),
roll_mean_negative = na_if(roll_mean_norm_negative_sum, 0),
positive_threshold = mean(roll_mean_positive,na.rm = TRUE) + z*sd(roll_mean_positive,na.rm = TRUE),
negative_threshold = mean(roll_mean_negative,na.rm = TRUE) + z*sd(roll_mean_negative,na.rm = TRUE)
) %>%
group_by(date,isin) %>%
# Make sure that we only calculate the cutoff value of periods with actual observations.
mutate(
neg_event =
case_when(roll_mean_negative > negative_threshold & roll_mean_negative > 5 #& roll_mean_negative > 0.5*roll_mean_norm_positive_sum
~ 1,
TRUE ~ 0)
)
# Add lagged values of the events to sort whether an event has happened in the last X months.
for (i in 1:12) {
long_term_data_monthly[paste0("-",i)] <- long_term_data_monthly %>% group_by(isin) %>% transmute(t = lag(neg_event,i)) %>% ungroup() %>% select(-isin)
}
long_term_data_negative = long_term_data_monthly %>% select(date,isin,ret,MKT_CAP,free_float_mkt_cap,contains("-")) %>%
pivot_longer(!c(date,isin,ret,MKT_CAP,free_float_mkt_cap), names_to = "period", values_to = "event") %>%
mutate(period = as.numeric(period)) %>%
filter(event == '1') %>%
# Remove duplicate rows of the returns in case one ISIN pops up on several dates.
group_by(date,isin) %>% distinct(ret, .keep_all = TRUE)
long_term_data_negative
long_term_data_negative %>% filter(period == '-1')
long_term_data_negative %>% filter(period == '-1') %>% arrange(date)
long_term_data_negative %>% filter(period == '-1') %>% disinct(date)
long_term_data_negative %>% filter(period == '-1') %>% summarise(disinct(date))
long_term_data_negative %>% filter(period == '-1') %>% unique(date)
long_term_data_negative %>% filter(period == '-1')
# STOXX 600 Europe index
sxxp_index_daily <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/STOXX_600_INDEX.csv",
sep = ",") %>%
transmute(date = as.Date(Date, format = "%m/%d/%Y"),
mkt_excess = as.numeric(gsub('%','',Change..))/100)
# Sentiment and stock data merged together with daily observations
data_daily <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/data_daily.csv", sep = ",")  %>%
mutate(date = as.Date(date)) %>% select(-X)
ESG_RR <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/ESG_ratings.csv") %>%
select("isin" = ISIN,ESG_risk_category) %>%
# mutate(ESG_risk_category =
#          case_when(
#   ESG_risk_category == "Severe" ~ "High",
#   ESG_risk_category == "Negligible" ~ "Low",
#   TRUE ~ ESG_risk_category)) %>%
filter(!ESG_risk_category %in% c('No data','Severe','Negligible')) %>% na.omit()
short_term_data_daily = data_daily %>%
mutate(date = as.Date(date)) %>%
right_join(sxxp_index_daily %>% select(date, mkt_excess), by = "date")
betas <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/nested_daily_for_capm_sxxp.csv", sep = ",") %>%
select(-X) %>%
mutate(date = as.Date(date))
betas <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/nested_daily_for_capm_sxxp.csv", sep = ",") %>%
select(-X) %>%
mutate(date = as.Date(date))
short_term_data <- betas %>%
left_join(short_term_data_daily, by = c("date","isin")) %>%
group_by(isin) %>%
mutate(
pred_ret = alpha + beta*mkt_excess,
abnormal_ret = W_RETURN - pred_ret,
norm_positive_sum = sentiment_positive_count - sdg_not_relevant_positive_count,
norm_negative_sum = sentiment_negative_count - sdg_not_relevant_negative_count
) %>%
select(-c(beta,alpha,pred_ret,sdg_not_relevant_positive_count,sdg_not_relevant_negative_count))
for (i in 1:10) {
short_term_data[paste0("-",i)] <- short_term_data %>% group_by(isin) %>% transmute(t = lag(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
short_term_data[paste0("+",i)] <- short_term_data %>% group_by(isin) %>% transmute(t = lead(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
short_term_data[paste0("neg_event_-",i)] <- short_term_data %>% group_by(isin) %>% transmute(l = lag(norm_negative_sum,i)) %>% ungroup() %>% select(-isin)
short_term_data[paste0("neg_event_+",i)] <- short_term_data %>% group_by(isin) %>% transmute(l = lead(norm_negative_sum,i)) %>% ungroup() %>% select(-isin)
short_term_data[paste0("pos_event_-",i)] <- short_term_data %>% group_by(isin) %>% transmute(l = lag(norm_positive_sum,i)) %>% ungroup() %>% select(-isin)
short_term_data[paste0("pos_event_+",i)] <- short_term_data %>% group_by(isin) %>% transmute(l = lead(norm_positive_sum,i)) %>% ungroup() %>% select(-isin)
}
short_term_data_event = short_term_data %>%
group_by(isin) %>%
mutate(
norm_positive_sum = na_if(norm_positive_sum, 0),
norm_negative_sum = na_if(norm_negative_sum, 0),
positive_threshold = mean(norm_positive_sum,na.rm = TRUE) + 1*sd(norm_positive_sum,na.rm = TRUE),
negative_threshold = mean(norm_negative_sum,na.rm = TRUE) + 1*sd(norm_negative_sum,na.rm = TRUE)
) %>%
rename("0" = abnormal_ret) %>%
select(-c("W_RETURN","mkt_excess",
#"positive_threshold","negative_threshold","norm_positive_sum","norm_negative_sum",
"sentiment_negative_count","sentiment_positive_count",
#"roll_mean_norm_negative_sum","roll_mean_norm_positive_sum", "roll_sum_norm_negative_sum","roll_sum_norm_positive_sum"
)) %>%
group_by(date,isin) %>%
# Make sure that we only calculate the cutoff value of periods with actual observations.
mutate(pos_event =
case_when(norm_positive_sum > positive_threshold & norm_positive_sum > 10 & norm_positive_sum > 2*norm_negative_sum ~ 1,
TRUE ~ 0),
neg_event =
case_when(norm_negative_sum > negative_threshold & norm_negative_sum > 10 & norm_negative_sum > 2*norm_positive_sum ~ 1,
TRUE ~ 0)
)
negative_index = short_term_data_event %>%
select(date,isin,MKT_CAP,"0",contains("-"),contains("+"),neg_event) %>%
select(-c(contains("pos_event_"),contains("neg_event_"))) %>%
filter(neg_event == 1) %>%
select(-neg_event) %>%
pivot_longer(!c(date,isin,MKT_CAP), names_to = "type", values_to = "value") %>%
na.omit(value)
negative_index
negative_index %>% filter(type == '0')
negative_index %>% left_join(ESG_RR, by = "isin") %>% filter(type == '0')
negative_index %>% left_join(ESG_RR, by = "isin") %>% filter(type == '0') %>% filter(ESG_risk_category == 'Low')
negative_index %>% left_join(ESG_RR, by = "isin") %>% filter(type == '0') %>% select(date,isin)
data_monthly
long_term_data_negative
test = negative_index %>% left_join(ESG_RR, by = "isin") %>% filter(type == '0') %>% select(date,isin)
long_term_data_negative %>%
right_join(test, by = "isin")
test
