---
title: "Speciale_analysis"
author: "Martin Andersen"
date: "2023-02-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r,message=FALSE}
# Load packages:
library(tidyverse)
library(lubridate)
library(readxl)
library(timetk)
library(dplyr)
library(zoo)
library(furrr)
library(ggplot2)
library(ggpubr)
library(xtable)
library(purrr)
library(stringr)
library(magrittr)

source("functions.R")
```


################ Portfolio sorts 1 week sentiment data:
First we download the data that we want to work with, and compute the market cap weighted mean
################
```{r}

# Data connection to get STOXX 600 Europe sentiment and stock data. Joined on stock data, so there will be NA's in the sentiment.  
data_weekly <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/all_data_weekly.csv", sep = ",") %>% 
  select(-c(X,observations_unique_sentences,observations_unique_articles,observations_unique_sources,global_unique_sentences,global_unique_articles,global_unique_sources,
            sentiment_negative_mean:sdg_broad_positive_count
            )) %>% 
  na.omit(observations) %>%
  group_by(date) %>% 
  mutate(scalar= round(1 + scale(MKT_CAP),2),
         negative_sum = round((negative_sum/scalar),2),
         neutral_sum = round((neutral_sum/scalar),2), 
         positive_sum = round((positive_sum/scalar),2)
         ) %>% arrange(date) 

# Data connection to get daily market returns for STOXX 600 Europe
msci_index_daily <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/MSCI_World_daily.csv", sep = ",") %>%
  mutate(date = as.Date(Date, format = "%m/%d/%Y"),
    mkt_excess = as.numeric(sub("%", "",Change..))/100)

sxxp_index_daily <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/STOXX600_index_daily.csv", sep = ",") %>%
  mutate(date = as.Date(Date, format = "%m/%d/%Y"),
    mkt_excess = as.numeric(sub("%", "",Change..))/100)

```


################ 
Univariate portfolio sorts 1 week sentiment data:
################
```{r}

sentiment_portfolios <- data_weekly %>% # filter(positive_sum > 0 | negative_sum > 0) %>%
  group_by(date) %>%
  mutate(
    breakpoint = mean(negative_sum),
    portfolio = case_when(
      negative_sum <= breakpoint ~ "low",
      negative_sum > breakpoint ~ "high"
    )
  ) |>
  group_by(date, portfolio) |>
  summarize(ret = weighted.mean(W_RETURN, MKT_CAP), .groups = "drop") %>%
  pivot_wider(date, names_from = portfolio, values_from = ret) |>
  mutate(ret = low - high)

portfolios_summary <- beta_portfolios %>% mutate(date = as.Date(date)) %>%
  left_join(stoxx600_index %>% select(date,mkt_excess), by = "date") %>%
  #group_by(portfolio) |>
  summarize(
    alpha = as.numeric(lm(ret ~ 1 + mkt_excess)$coefficients[1]),
    beta = as.numeric(lm(ret ~ 1 + mkt_excess)$coefficients[2]),
    ret = 54*mean(mkt_excess)
  )

```


################
Bivariate sorts
################
```{r}

value_portfolios <- data_weekly %>% filter(positive_sum > 0 | negative_sum > 0) %>%
  group_by(date) %>%
  
  mutate(portfolio_pos = assign_portfolio_bi(
    data = cur_data(),
    var = positive_sum,
    n_portfolios = 3
  )) %>%
  
  group_by(date, portfolio_pos) %>%
  mutate(
    portfolio_neg = assign_portfolio_bi(
      data = cur_data(),
      var = negative_sum,
      n_portfolios = 3
    ),
    
    portfolio_combined = str_c(portfolio_neg, portfolio_pos)
  ) %>% 
  
  select(date,isin,portfolio_neg,portfolio_pos,portfolio_combined,W_RETURN,MKT_CAP,negative_sum) %>% filter(portfolio_neg == 1)
  group_by(date, portfolio_combined) %>%
  summarize(
    ret = weighted.mean(W_RETURN, MKT_CAP),
    portfolio_pos = unique(portfolio_pos),
    .groups = "drop"
  )

value_portfolios %>% 
  group_by(portfolio_combined) %>%
  summarise(ret = mean(ret))

# 
# value_premium <- value_portfolios %>%
#   group_by(date, portfolio_pos) %>%
#   summarize(ret = mean(ret), .groups = "drop_last") %>%
#   summarize(value_premium = ret[portfolio_pos == max(portfolio_pos)] -
#     ret[portfolio_pos == min(portfolio_pos)])
# 
# mean(value_premium$value_premium * 100)
  short_term_data_event %>%
  filter(pos_event == 1)
```




################
Short term abnormal return after an individual event - Daily
################
```{r}
data_daily <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/data_daily.csv", sep = ",")  %>% 
  select(date,isin,W_RETURN,negative_sum,positive_sum) %>%
  mutate(date = as.Date(date)) %>% 
  right_join(sxxp_index_daily %>% select(date, mkt_excess), by = "date")
  

plan(multisession, workers = availableCores())

short_term_data_nested <- data_daily %>% ungroup() %>% 
  select(date,isin,W_RETURN,mkt_excess) %>%
  nest(data = -c(isin)) %>%
  mutate(beta = future_map(
    data, ~ roll_capm_estimation(., window = 120, min_obs = 60, period = "day")
  )) %>%
  unnest(c(beta)) %>%
  na.omit(beta,alpha) %>% select(-data)

# # Write to CSV to keep in nice format:
        # short_term_data_nested %>%
        #   write.csv(file = "C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/nested_daily_for_capm_sxxp.csv")

 short_term_data_nested <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/nested_daily_for_capm.csv", sep = ",") %>%
   select(-X) %>%
   mutate(date = as.Date(date))
 
 sentiment_daily <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/clean_sentiment_daily.csv", sep = ",") %>%
   select(-X) %>%
   mutate(date = as.Date(date))

 
 
short_term_data <- short_term_data_nested %>% 
  left_join(data_daily, by = c("date","isin")) %>%
  
  left_join(
    sentiment_daily %>%
      mutate(date = as.Date(date)) %>%
      select(date,isin,sdg_not_relevant_negative_count,sdg_not_relevant_positive_count,sentiment_negative_count,sentiment_positive_count), 
            by =c("date","isin")) %>%
  
  group_by(isin) %>%
  mutate(
      pred_ret = alpha + beta*mkt_excess,
      abnormal_ret = W_RETURN - pred_ret,
      norm_positive_sum = sentiment_positive_count - sdg_not_relevant_positive_count,
      norm_negative_sum = sentiment_negative_count - sdg_not_relevant_negative_count#,
  #     
  #     roll_sum = across(!c(date,sdg_not_relevant_positive_count,sdg_not_relevant_negative_count,beta,alpha,W_RETURN,negative_sum,
  #                     positive_sum,mkt_excess,pred_ret,abnormal_ret,sentiment_positive_count,sentiment_negative_count),
  #                     ~ rollsum(.x, k = 5, fill = NA, align = "right"))) %>% 
  # unnest(roll_sum,names_sep = "_") %>%
  # 
  # mutate(
  #   roll_mean = across(!c(date,sdg_not_relevant_positive_count,sdg_not_relevant_negative_count,beta,alpha,W_RETURN,negative_sum,
  #                      positive_sum,mkt_excess,pred_ret,abnormal_ret,roll_sum_norm_positive_sum,roll_sum_norm_negative_sum,
  #                      sentiment_positive_count,sentiment_negative_count),
  #                     ~ rollmean(.x, k = 5, fill = NA, align = "right"))
  #   
     
  ) %>%
  #unnest(roll_mean,names_sep = "_") %>%
  select(-c(beta,alpha,pred_ret,sdg_not_relevant_positive_count,sdg_not_relevant_negative_count))

# Create columns that represent the lagged and leading values in relation to the event date:

for (i in 1:10) {
  short_term_data[paste0("-",i)] <- short_term_data %>% group_by(isin) %>% transmute(t = lag(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
  short_term_data[paste0("+",i)] <- short_term_data %>% group_by(isin) %>% transmute(t = lead(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
}

short_term_data_event = short_term_data %>% 
  group_by(isin) %>%
  mutate(
    
    # norm_positive_sum = roll_mean_norm_positive_sum,
    # norm_negative_sum = roll_mean_norm_negative_sum,
    
    norm_positive_sum = na_if(norm_positive_sum, 0),
    norm_negative_sum = na_if(norm_negative_sum, 0),
    
    positive_sd = sd(norm_positive_sum,na.rm = TRUE),
    negative_sd = sd(norm_negative_sum,na.rm = TRUE),
    
    positive_threshold = mean(norm_positive_sum,na.rm = TRUE) + 2*positive_sd,
    negative_threshold = mean(norm_negative_sum,na.rm = TRUE) + 1*negative_sd
    ) %>%
  
   select(-c("W_RETURN","mkt_excess","positive_sum","negative_sum","positive_sd","negative_sd",
             #"positive_threshold","negative_threshold","norm_positive_sum","norm_negative_sum",
              "sentiment_negative_count","sentiment_positive_count",
             #"roll_mean_norm_negative_sum","roll_mean_norm_positive_sum", "roll_sum_norm_negative_sum","roll_sum_norm_positive_sum"
             )) %>%
  rename("0" = abnormal_ret) %>%

  group_by(date,isin) %>%
  # Make sure that we only calculate the cutoff value of periods with actual observations. 
  mutate(pos_event = 
    case_when(norm_positive_sum > positive_threshold & norm_positive_sum > 10 & norm_positive_sum > 2*norm_negative_sum ~ 1,
              TRUE
               ~ 0),
    neg_event = 
    case_when(norm_negative_sum > negative_threshold & norm_negative_sum > 10 & norm_negative_sum > 2*norm_positive_sum ~ 1, TRUE
               ~ 0)
    ) 

# Removing outliers:

  pos_data = short_term_data_event %>%
  filter(pos_event == 1) %>% na.omit(3)
 pos_data = pos_data[!(is.na(pos_data$'0')), ]
quartiles <- quantile(pos_data$'0', probs=c(.25, .75), na.rm = TRUE)
IQR <- IQR(pos_data$'0')
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
pos_data_no_outlier <- subset(pos_data, pos_data$'0' > Lower & pos_data$'0' < Upper)


  neg_data = short_term_data_event %>%
  filter(neg_event == 1) %>% na.omit(3)
 neg_data = neg_data[!(is.na(neg_data$'0')), ]
quartiles <- quantile(neg_data$'0', probs=c(.25, .75), na.rm = TRUE)
IQR <- IQR(neg_data$'0')
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
neg_data_no_outlier <- subset(neg_data, neg_data$'0' > Lower & neg_data$'0' < Upper)

#Splitting events into positive and negative, and cleaning the data:

positive_index = pos_data %>% 
  select(-c(pos_event,neg_event,positive_threshold,negative_threshold,norm_positive_sum,norm_negative_sum)) %>%
  pivot_longer(!c(date,isin), names_to = "type", values_to = "value") %>%
  na.omit(value) %>%
  mutate(type = as.factor(type)) %>%
  group_by(type) %>% 
  summarise(AAR = mean(value),
            SE = sd(value)/sqrt(n())) %>% 
  mutate(event = "positive",
         type = factor(type, levels = c("-10", "-9","-8","-7","-6","-5","-4","-3","-2","-1","0", "+1", "+2","+3","+4","+5","+6","+7","+8","+9","+10"))) %>%
  arrange(type) %>%
  mutate(CAAR = cumsum(AAR)) 


negative_index = neg_data %>%
  select(-c(pos_event,neg_event,positive_threshold,negative_threshold,norm_positive_sum,norm_negative_sum)) %>%
  pivot_longer(!c(date,isin), names_to = "type", values_to = "value") %>%
  na.omit(value) %>%
  mutate(type = as.factor(type)) %>%
  group_by(type) %>% 
  summarise(AAR = mean(value),
            SE = sd(value)/sqrt(n())) %>%
  mutate( event = "negative",
    type = factor(type, levels = c("-10", "-9","-8","-7","-6","-5","-4","-3","-2","-1","0", "+1", "+2","+3","+4","+5","+6","+7","+8","+9","+10"))) %>%
  arrange(type) %>%
  mutate(CAAR = cumsum(AAR))
  

# Plots

positive_index %>%  
  pivot_longer(!c(type,event,SE), names_to = "metric", values_to = "value") %>%
  group_by(metric) %>%
  mutate(m = mean(value, na.rm = TRUE),
         s = sd(value, na.rm = TRUE),
         hi = value + m + 2*SE,
         lo = value - m - 2*SE) %>% 
  
  ggplot(aes(x = type, y = value, colour = metric, group = metric)) + 
  geom_ribbon(aes(ymin = lo, ymax = hi, colour = metric, group = metric, fill = metric), 
              alpha=0.5, 
              linetype="dashed") +
  geom_line(size=1.2) +
  theme_bw() +
    theme(legend.title = element_blank(),
          legend.position="top",
          legend.direction = "horizontal") +
  scale_y_continuous(name ="Abnormal return",
                     labels = scales::percent) + 
  scale_x_discrete(name = "Relative time",breaks=c("-10","-5","0","+5","+10"))
ggsave("ST_positive_all_CI.png")

negative_index %>%  
  pivot_longer(!c(type,event,SE), names_to = "metric", values_to = "value") %>%
  group_by(metric) %>%
  mutate(m = mean(value, na.rm = TRUE),
         s = sd(value, na.rm = TRUE),
         hi = value + m + 2*SE,
         lo = value - m - 2*SE) %>% 
  ggplot(aes(x = type, y = value, colour = metric, group = metric)) + 
  geom_ribbon(aes(ymin = lo, ymax = hi, colour = metric, group = metric, fill = metric), 
              alpha=0.5, 
              linetype="dashed") +
  geom_line(size=1.2) +
  theme_bw() +
  theme(legend.title = element_blank(),
        axis.title.y = element_blank(),
        legend.position="top",
        legend.direction = "horizontal") +
  scale_y_continuous(name ="Abnormal return",
                     labels = scales::percent) + 
  scale_x_discrete(name = "Relative time",breaks=c("-10","-5","0","+5","+10"))
ggsave("ST_negative_all_CI.png")


# Tables to Overleaf


positive_table = positive_index %>% 
  select(type,AAR,CAAR) %>%
  mutate(AAR = 100*AAR,
         CAAR = 100*CAAR) %>%
  arrange(type)

negative_table = negative_index %>% 
  mutate(AAR = 100*AAR,
         CAAR = 100*CAAR) %>%
  arrange(type) %>%  select(AAR,CAAR)

st_table = cbind(positive_table,negative_table)

addtorow <- list() 
addtorow$pos <- list(0, 0) 
addtorow$command <- c(" & \\multicolumn{2}{c}{Positive news} & \\multicolumn{2}{c}{Negative news}  \\\\\n", " Time & AAR & CAAR & AAR & CAAR \\\\\n" )

print(xtable(st_table,type="latex",
             caption="AAR and CAAR over event window (in percentage)",
             digits=c(0,0,4,4,4,4),
             hline.after=c(-1,0),
             floating=FALSE,latex.environments=NULL),
      add.to.row = addtorow,
      include.rownames=FALSE,
      include.colnames = FALSE) %>% 
  write(file="test.tex")


  
```


################
Short term abnormal return after an individual event - Daily split on SDG's
################
```{r}
 # Use the data from above chunk

short_term_data <- short_term_data_nested %>% 
  left_join(data_daily, by = c("date","isin")) %>%
  
  left_join(
    sentiment_daily %>%
      mutate(date = as.Date(date)),
      # select(date,isin,sdg_not_relevant_negative_count,sdg_not_relevant_positive_count,sentiment_negative_count,sentiment_positive_count), 
            by = c("date","isin")) %>%
  
  group_by(isin) %>%
  mutate(
      pred_ret = alpha + beta*mkt_excess,
      abnormal_ret = W_RETURN - pred_ret
  ) %>%
  select(-c(beta,alpha,pred_ret,sdg_not_relevant_positive_count,sdg_not_relevant_negative_count))


short_term_data_event = short_term_data %>% 
  select(isin,date,abnormal_ret, contains("sdg")) %>% 
  select(isin,date,abnormal_ret, contains("tive"),-c(sdg_broad_negative_count,sdg_broad_neutral_count,sdg_broad_positive_count))

# Create columns that represent the lagged and leading values in relation to the event date:

for (i in 1:10) {
  short_term_data_event[paste0("-",i)] <- short_term_data_event %>% group_by(isin) %>% transmute(t = lag(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
  short_term_data_event[paste0("",i)] <- short_term_data_event %>% group_by(isin) %>% transmute(t = lead(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
}

# Create the threshold and filter only firms that have a significant event:

short_term_return = short_term_data_event %>%
  pivot_longer(!c(date,isin,abnormal_ret,'-1':'10'), names_to = "sdg", values_to = "value") %>%
  group_by(isin,sdg) %>%
  mutate(
      value = na_if(value, 0),
      sd = sd(value,na.rm = TRUE),
      mean = mean(value, na.rm = TRUE),
      
    threshold = mean + 2*sd
  ) %>% 
  mutate(event = 
    case_when(value > threshold & value > 1 ~ 1,
              TRUE 
               ~ 0)
    ) %>%
  rename('0' = abnormal_ret) %>%
  select(-c(value,sd,mean,threshold)) %>% 
  filter(event == 1) %>%
  mutate(group = case_when(grepl("pos", sdg) ~ "positive",
                            grepl("nega", sdg) ~"negative"),
         ) 
  

# Splitting up positive and negative events:

positive_index = short_term_return %>%
  filter(group == 'positive') %>%
  mutate_at("sdg", str_replace, "_positive_count", "") %>%
  mutate_at("sdg", str_replace, "_", " ") %>%
  mutate(sdg = factor(sdg, levels = c("sdg 1", "sdg 2","sdg 3","sdg 4","sdg 5","sdg 6","sdg 7","sdg 8","sdg 9","sdg 10","sdg 11", 
                                        "sdg 12", "sdg 13","sdg 14","sdg 15","sdg 16","sdg 17"))) %>%
  select(-c(event,group)) %>%
  pivot_longer(!c(date,isin,sdg), names_to = "period", values_to = "abnormal_ret") %>% na.omit(abnormal_ret) %>%
  group_by(sdg,period) %>%
    mutate(period = as.factor(period)) %>%
  summarise(AAR = mean(abnormal_ret)) %>%
  mutate( event = "positive",
    period = factor(period, levels = c("-10", "-9","-8","-7","-6","-5","-4","-3","-2","-1","0", "1", "2","3","4","5","6","7","8","9","10"))) %>%
  arrange(sdg,period) %>%
  mutate(CAAR = cumsum(AAR))


negative_index = short_term_return %>%
  filter(group == 'negative') %>%
  mutate_at("sdg", str_replace, "_negative_count", "") %>%
  mutate_at("sdg", str_replace, "_", " ") %>%
  mutate(sdg = factor(sdg, levels = c("sdg 1", "sdg 2","sdg 3","sdg 4","sdg 5","sdg 6","sdg 7","sdg 8","sdg 9","sdg 10","sdg 11", 
                                        "sdg 12", "sdg 13","sdg 14","sdg 15","sdg 16","sdg 17"))) %>%
  select(-c(event,group)) %>%
  pivot_longer(!c(date,isin,sdg), names_to = "period", values_to = "abnormal_ret") %>% na.omit(abnormal_ret) %>%
  group_by(sdg,period) %>%
    mutate(period = as.factor(period)) %>%
  summarise(AAR = mean(abnormal_ret)) %>%
  mutate( event = "negative",
    period = factor(period, levels = c("-10", "-9","-8","-7","-6","-5","-4","-3","-2","-1","0", "1", "2","3","4","5","6","7","8","9","10"))) %>%
  arrange(sdg,period) %>%
  mutate(CAAR = cumsum(AAR))


# Plots of all SDGs

positive_index %>% 
  ggplot(aes(x = period, y = CAAR, group = factor(sdg), colour = factor(sdg))) + 
  geom_line() +
  theme_bw() +
    theme(legend.title = element_blank(),
          legend.key.size = unit(0.5, 'cm')) +
  scale_y_continuous(name ="Abnormal return",
                     labels = scales::percent) + 
  scale_x_discrete(name = "Relative time",breaks=c("-10","-5","0","5","10"))
ggsave("ST_positive_sdgs.png")

  
negative_index %>% 
  ggplot(aes(x = period, y = CAAR, group = factor(sdg), colour = factor(sdg))) + 
  geom_line() +
  theme_bw() +
    theme(legend.title = element_blank(),
          legend.key.size = unit(0.5, 'cm')) +
  scale_y_continuous(name ="Abnormal return",
                     labels = scales::percent) + 
  scale_x_discrete(name = "Relative time",breaks=c("-10","-5","0","5","10"))
ggsave("ST_negative_sdgs.png")

## Bar plot of all SDGs

n_positive = short_term_return %>%
  filter(group == 'positive') %>%
  mutate_at("sdg", str_replace, "_positive_count", "") %>%
  mutate_at("sdg", str_replace, "_", " ") %>%
  mutate(sdg = factor(sdg, levels = c("sdg 1", "sdg 2","sdg 3","sdg 4","sdg 5","sdg 6","sdg 7","sdg 8","sdg 9","sdg 10","sdg 11", 
                                        "sdg 12", "sdg 13","sdg 14","sdg 15","sdg 16","sdg 17"))) %>%
  group_by(sdg) %>% summarise(n = n())

positive_index %>% 
  group_by(sdg) %>% 
  summarise(
  sd = sd(AAR),
  sum = sum(AAR)) %>%
  left_join(n_positive, by = "sdg") %>%
  ggplot(
    aes(x = sdg, y = sum, group = sdg, fill = sdg)) + 
  geom_bar(stat="identity") +
  geom_text(aes(label=n),  color="black", size=3.5,
            vjust=1.5) +
  scale_y_continuous(name ="Abnormal return",
                     labels = scales::percent,
                     limits=c(-0.01, 0.005)) +
  theme(legend.title = element_blank(),
        legend.key.size = unit(0.5, 'cm'),
        axis.text.x = element_text(angle=45))
ggsave("ST_positive_sdg_bar.png")

n_negative = short_term_return %>%
  filter(group == 'negative') %>%
  mutate_at("sdg", str_replace, "_negative_count", "") %>%
  mutate_at("sdg", str_replace, "_", " ") %>%
  mutate(sdg = factor(sdg, levels = c("sdg 1", "sdg 2","sdg 3","sdg 4","sdg 5","sdg 6","sdg 7","sdg 8","sdg 9","sdg 10","sdg 11", 
                                        "sdg 12", "sdg 13","sdg 14","sdg 15","sdg 16","sdg 17"))) %>% 
    group_by(sdg) %>% summarise(n = n())

negative_index %>% 
  group_by(sdg) %>% 
  summarise(
  sd = sd(AAR),
  sum = sum(AAR)) %>%
  left_join(n_negative, by = "sdg") %>%
  ggplot(
    aes(x = sdg, y = sum, group = sdg, fill = sdg)) + 
  geom_bar(stat="identity") +
  geom_text(aes(label=n),  color="black", size=3.5,
            vjust=1.5) +
  scale_y_continuous(name ="Abnormal return",
                     labels = scales::percent,
                     limits=c(-0.01, 0.02)) +
  theme(legend.title = element_blank(),
        legend.key.size = unit(0.5, 'cm'),
        axis.text.x = element_text(angle=45))
ggsave("ST_negative_sdg_bar.png")


```



Keep working on this for the initial overview:
```{r}

long_term_daily_data <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/data_daily.csv", sep = ",")  %>% 
  select(date,isin,W_RETURN,PX_LAST,MKT_CAP,negative_sum,positive_sum) %>%
  mutate(date = as.Date(date)) %>% 
  right_join(msci_index_daily %>% select(date, mkt_excess), by = "date") %>%
   mutate(abnormal_ret = W_RETURN - mkt_excess) 

# Create columns that represent the lagged and leading values in relation to the event date:
for (i in 1:150) {
  long_term_daily_data[paste0("-",i)] <- long_term_daily_data %>% group_by(isin) %>% transmute(t = lag(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
  long_term_daily_data[paste0("+",i)] <- long_term_daily_data %>% group_by(isin) %>% transmute(t = lead(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
}

short_term_data_event = short_term_data %>% 
  group_by(isin) %>%
  mutate(
    positive_sum = na_if(positive_sum, 0),
    negative_sum = na_if(negative_sum, 0),
    positive_mean = mean(positive_sum,na.rm = TRUE),
    negative_mean = mean(negative_sum,na.rm = TRUE)) %>%
  group_by(date,isin) %>%
  # Make sure that we only calculate the cutoff value of periods with actual observations. 
  mutate(pos_event = 
    case_when(positive_sum > positive_mean ~ 1,
              TRUE
               ~ 0),
    neg_event = 
    case_when(negative_sum > negative_mean ~ 1, TRUE
               ~ 0)
    ) 

short_term_data_event %>%
  filter(pos_event == 1 | neg_event == 1) %>%
  ungroup() %>% 
  rename("0" = abnormal_ret) %>% 
  # Remove columns we don't use
  select(-c("W_RETURN","PX_LAST","mkt_excess","positive_sum","MKT_CAP","negative_sum","event","positive_mean","pos_mean")) %>%
  pivot_longer(!c(date,isin), names_to = "type", values_to = "value") %>% 
  na.omit(value) %>%
  group_by(isin,type) %>% 
  summarise(abnormal_ret = mean(value))


lagged_index = short_term_data_event %>%
  mutate(type = as.character(type),
         type = as.integer(type)) %>% filter(type <= 0) %>% 
  mutate(type = as.factor(type)) 

lagged_index$type <- factor(lagged_index$type, levels = c("0", "-1", "-2","-3","-4","-5","-6","-7","-8","-9","-10"))

lagged_index = lagged_index %>%
  arrange(isin,type) %>%
  group_by(isin) %>%
  mutate(index_ret = case_when(type == '0' ~ 0,
                        TRUE ~ abnormal_ret),
         index =  100*(cumprod(1+index_ret)))

lead_index = short_term_data_event %>%
  mutate(type = as.character(type),
         type = as.integer(type)) %>% filter(type > 0) %>% 
  mutate(type = as.factor(type)) 

lead_index$type <- factor(lead_index$type, levels = c("0", "1", "2","3","4","5","6","7","8","9","10"))

lead_index = lead_index %>%
  arrange(isin,type) %>%
  group_by(isin) %>%
  mutate(index_ret = case_when(type == '0' ~ 0,
                        TRUE ~ abnormal_ret),
         index =  100*(cumprod(1+index_ret)))

short_term_indexed = lead_index %>%
  rbind(lagged_index)

short_term_indexed$type <- factor(short_term_indexed$type, levels = c("-10", "-9","-8","-7","-6","-5","-4","-3","-2","-1","0", "1", "2","3","4","5","6","7","8","9","10"))

short_term_indexed %>% 
  ggplot(aes(x = type, y = index, colour = isin, group = isin)) + 
  geom_line() +
  theme(legend.position = "none")


short_term_indexed %>%
  mutate(type =)
  group_by(type) %>%
  summarise(AAR =  mean(abnormal_ret)) %>% 
  ggplot(aes(x = type, y = AAR)) +
  geom_point() +
  theme(legend.position = "none")

```


################
Short term abnormal return after an individual event - Weekly
################
```{r}
data_weekly <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/all_data_weekly.csv", sep = ",") %>%
  select(date,isin,W_RETURN,PX_LAST,MKT_CAP,negative_sum,positive_sum) %>% 
  mutate(date = as.Date(date)) %>%
  left_join(stoxx600_index %>% select(date, mkt_excess), by = "date")

short_term_data_nested <- data_weekly %>% ungroup() %>%
  select(date,isin,W_RETURN,mkt_excess) %>%
  nest(data = -c(isin)) %>%
  mutate(beta = future_map(
    data, ~ roll_capm_estimation(.,weeks = 10, min_obs = 10)
  )) %>%
  unnest(c(beta)) %>%
  na.omit(beta,alpha) %>% select(-data)

short_term_data <- short_term_data_nested %>% 
  left_join(data_weekly, by = c("date","isin")) %>%
  mutate(
    pred_ret = alpha + beta*mkt_excess,
    abnormal_ret = W_RETURN - pred_ret) %>%
  select(-c(beta,alpha,pred_ret))

# Create columns that represent the lagged and leading values in relation to the event date:
for (i in 1:10) {
  
  short_term_data[paste0("-",i)] <- short_term_data %>% group_by(isin) %>% transmute(t = lag(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
  
  short_term_data[paste0("+",i)] <- short_term_data %>% group_by(isin) %>% transmute(t = lead(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
}


short_term_data_event = short_term_data %>%
  group_by(isin) %>%
  mutate(
    pos_mean = na_if(positive_sum, 0),
    positive_mean = mean(pos_mean,na.rm = TRUE)) %>%
  group_by(date,isin) %>%
  # Make sure that we only calculate the cutoff value of periods with actual observations. 
  mutate(event = 
    case_when(positive_sum > positive_mean ~ 1, TRUE
               ~ 0)
  ) %>%
  filter(event == 1) %>%
  ungroup() %>% 
  rename("0" = abnormal_ret) %>% 
  # Remove columns we don't use
  select(-c("W_RETURN","PX_LAST","mkt_excess","positive_sum","MKT_CAP","negative_sum","event","positive_mean","pos_mean")) %>%
  pivot_longer(!c(date,isin), names_to = "type", values_to = "value") %>% 
  na.omit(value) %>%
  group_by(isin,type) %>% 
  summarise(abnormal_ret = mean(value))


lagged_index = short_term_data_event %>%
  mutate(type = as.character(type),
         type = as.integer(type)) %>% filter(type <= 0) %>% 
  mutate(type = as.factor(type)) 

lagged_index$type <- factor(lagged_index$type, levels = c("0", "-1", "-2","-3","-4","-5","-6","-7","-8","-9","-10"))

lagged_index = lagged_index %>%
  arrange(isin,type) %>%
  group_by(isin) %>%
  mutate(index_ret = case_when(type == '0' ~ 0,
                        TRUE ~ abnormal_ret),
         index =  100*(cumprod(1+index_ret)))

lead_index = short_term_data_event %>%
  mutate(type = as.character(type),
         type = as.integer(type)) %>% filter(type > 0) %>% 
  mutate(type = as.factor(type)) 

lead_index$type <- factor(lead_index$type, levels = c("0", "1", "2","3","4","5","6","7","8","9","10"))

lead_index = lead_index %>%
  arrange(isin,type) %>%
  group_by(isin) %>%
  mutate(index_ret = case_when(type == '0' ~ 0,
                        TRUE ~ abnormal_ret),
         index =  100*(cumprod(1+index_ret)))

short_term_indexed = lead_index %>%
  rbind(lagged_index)

short_term_indexed$type <- factor(short_term_indexed$type, levels = c("-10", "-9","-8","-7","-6","-5","-4","-3","-2","-1","0", "1", "2","3","4","5","6","7","8","9","10"))

short_term_indexed %>% 
  ggplot(aes(x = type, y = index, colour = isin, group = isin)) + 
  geom_line() +
  theme(legend.position = "none")


short_term_indexed %>%
  group_by(type) %>%
  summarise(AAR =  mean(54 * abnormal_ret)) %>% 
  ggplot(aes(x = type, y = AAR)) +
  geom_point() +
  theme(legend.position = "none")
```
