---
title: "Speciale_analysis"
author: "Martin Andersen"
date: "2023-02-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r,message=FALSE}
# Load packages:
library(tidyverse)
library(lubridate)
library(readxl)
library(lmtest)
library(timetk)
library(dplyr)
library(zoo)
library(furrr)
library(ggplot2)
library(ggpubr)
library(xtable)
library(purrr)
library(stringr)
library(magrittr)
library(BSDA)
library(scales)
library(generics)

source("functions.R")



```

#################################
### Data for daily returns:
#################################
```{r}

#  Nasdaq index 
index_daily <- read_excel("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/MSCI_World_daily.xlsx") %>%
  transmute(date = as.Date(Date, format = "%Y-%m-%d"),
         mkt_excess = as.numeric(gsub('%','',Change))/100)

# Sentiment and stock data merged together with daily observations
data_daily <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/data_daily_nasdaq.csv", sep = ",")  %>% 
  mutate(date = as.Date(date)) %>% select(-X)

isin_nasdaq = read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/nasdaq_isin.csv") %>% select(-X)

mkt_cap <- read_excel("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/World_market_cap.xlsx")  %>%
  pivot_longer(!date, names_to = "firm", values_to = "MKT_CAP") %>%  
  mutate(date = as.Date(date, format = "%Y-%m-%d"),
         date = ceiling_date(date, "month")-1) %>% 
  group_by(date,firm) %>%
  summarise(MKT_CAP = mean(MKT_CAP, na.rm = TRUE)) %>%
  left_join(ISIN, by = "firm") %>%
  transmute(isin = id_isin, MKT_CAP)


```

#################################
### Data for monthly returns:
#################################
```{r} 
# Data connection to get STOXX 600 Europe sentiment and stock data. Joined on stock data, so there will be NA's in the sentiment.  
 
# Sentiment and stock data merged together with weekly observations
data_monthly <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/all_data_month_nasdaq.csv", sep = ",") %>%
  mutate(date = as.Date(date)) %>% select(date,isin, MKT_CAP,W_RETURN = ret, sentiment_negative_count,sentiment_positive_count,sdg_not_relevant_negative_count, sdg_not_relevant_positive_count)

# In the Fama French dataset, the variable "mkt_excess_ret" is created from the Bloomberg data as a market cap-weighted index. 
dev_5factors_monthly <- read_excel("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/Developed_5_Factors.xlsx") %>% 
     transmute(
        date = as.Date(ym(date)),
        date = ceiling_date(date, "month")-1,
        SMB = as.numeric(SMB)/100,
        HML = as.numeric(HML)/100,
        RMW = as.numeric(RMW)/100,
        CMA = as.numeric(CMA)/100,
        RF = as.numeric(RF)/100,
        mkt_excess = as.numeric(Mkt)-RF) %>%
  filter(date >= '2018-01-01')

dev_5factors_monthly <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/Developed_5_Factors.csv") %>% 
     transmute(
        date = as.Date(ym(date)),
        date = ceiling_date(date, "month")-1,
        SMB = as.numeric(SMB)/100,
        HML = as.numeric(HML)/100,
        RMW = as.numeric(RMW)/100,
        CMA = as.numeric(CMA)/100,
        RF = as.numeric(RF)/100,
        mkt_excess = as.numeric(Mkt.RF)/100) %>%
  filter(date >= '2018-01-01')

```

################ 
Calendar-time portfolio approach 1 month sentiment data: 
################

### Positive news
```{r}
rolling_means = list(1)
SDs = list(1,2,3)
emptylist_pos_1 = list()
emptylist_pos_2 = list()

for (y in 1:length(rolling_means)) {
  
  for (z in 1:length(SDs)) { 

long_term_data_monthly <- data_monthly %>%
  transmute(date,isin,MKT_CAP, free_float_mkt_cap,
            ret = W_RETURN, 
            norm_positive_sum = sentiment_positive_count - sdg_not_relevant_positive_count,
            norm_negative_sum = sentiment_negative_count - sdg_not_relevant_negative_count,
            ) %>%
  group_by(isin) %>%
  transmute(date,isin,ret,MKT_CAP,free_float_mkt_cap,
    norm_positive_sum = replace_na(norm_positive_sum,0),
    norm_negative_sum = replace_na(norm_negative_sum,0),
    roll_mean = across(!c(date,ret,MKT_CAP,free_float_mkt_cap),
                      ~ rollmean(.x, k = y, fill = NA, align = "right"))) %>%
  unnest(roll_mean,names_sep = "_") %>% arrange(isin) %>%
  group_by(isin) %>%
    mutate(
    roll_mean_positive = na_if(roll_mean_norm_positive_sum, 0), 
    roll_mean_negative = na_if(roll_mean_norm_negative_sum, 0),
    positive_threshold = mean(roll_mean_positive,na.rm = TRUE) + z*sd(roll_mean_positive,na.rm = TRUE),
    negative_threshold = mean(roll_mean_negative,na.rm = TRUE) + 2*sd(roll_mean_negative,na.rm = TRUE)
    ) %>%
  group_by(date,isin) %>%
  # Make sure that we only calculate the cutoff value of periods with actual observations. 
  mutate(
    neg_event = 
    case_when(roll_mean_positive > positive_threshold & roll_mean_positive > 10 & roll_mean_positive > 2*roll_mean_norm_negative_sum 
              ~ 1, 
              TRUE ~ 0)
    ) 

############################
# Positive events:
############################

# Add lagged values of the events to sort whether an event has happened in the last X months. 
  for (i in 1:12) {
  long_term_data_monthly[paste0("-",i)] <- long_term_data_monthly %>% group_by(isin) %>% transmute(t = lag(neg_event,i)) %>% ungroup() %>% select(-isin)
  }

long_term_data_positive = long_term_data_monthly %>% select(date,isin,ret,MKT_CAP,free_float_mkt_cap,contains("-")) %>%
  pivot_longer(!c(date,isin,ret,MKT_CAP,free_float_mkt_cap), names_to = "period", values_to = "event") %>%
  mutate(period = as.numeric(period)) %>% 
  filter(event == '1') %>%
    # Remove duplicate rows of the returns in case one ISIN pops up on several dates. 
  group_by(date,isin) %>% distinct(ret, .keep_all = TRUE)


# x Month portfolio returns:

list = list(1,4,8,12)
alpha_positive <- data.frame(matrix(nrow=4, ncol=6)) 
colnames(alpha_positive)<-c("Alpha","t","p","r","N","model")

for (x in 1:length(list)) {
  
  i = list[[x]]

long_term_data_M_positive = long_term_data_positive %>%
  # Only include the last 1 months:
  filter(period >= -i) %>% 
  group_by(date) %>%
  summarise(
    ret = weighted.mean(ret,free_float_mkt_cap),
    #ret = mean(ret),        
    n = n_distinct(isin)) %>%
  left_join(eu_5factors_monthly, by = "date")

# Calculate alpha of portfolios:
  
alpha_M_positive = long_term_data_M_positive %>% 
  summarise(alpha = tidy(lm(ret-RF~ 1 + mkt_excess_ret + SMB + HML + RMW + CMA, weights = n))$estimate[1],
             t = tidy(lm(ret-RF~ 1 + mkt_excess_ret + SMB + HML + RMW + CMA, weights = n))$statistic[1],
             p = tidy(lm(ret-RF~ 1 + mkt_excess_ret + SMB + HML + RMW + CMA, weights = n))$p.value[1],
            r = glance(lm(ret-RF~ 1 + mkt_excess_ret + SMB + HML + RMW + CMA, weights = n))$r.squared,
             N = mean(n)) %>%
   mutate(model = paste0(i,"M_",y,"_",z))

alpha_positive[x,] = alpha_M_positive

}
emptylist_pos_1[[z]] = alpha_positive
}
emptylist_pos_2[[y]] = emptylist_pos_1
}



print(xtable(as.data.frame(emptylist_pos_2[[1]][1]),type="latex",
             digits=c(0,4,2,3,3,2,0),
             hline.after=c(-1,0),
             floating=FALSE,latex.environments=NULL),
      include.rownames=FALSE,
      include.colnames = TRUE) %>% 
  write(file="alpha_positive_FF5_RF1.tex")

tidy(summary(lm(ret - RF ~ 1 + mkt_excess_ret + SMB + HML + RMW + CMA, weights = n, data = long_term_data_M_positive)))

```

### Negative news
```{r}

rolling_means = list(1)
SDs = list(1)
emptylist_neg_1 = list()
emptylist_neg_2 = list()


for (y in 1:length(rolling_means)) {
  
  for (z in 1:length(SDs)) { 

long_term_data_monthly <- data_monthly %>%
  left_join(ESG_RR, by ="isin") %>% filter(ESG_risk_category == 'High') %>%
  transmute(date,isin,MKT_CAP,
            ret = W_RETURN, 
            norm_positive_sum = sentiment_positive_count - sdg_not_relevant_positive_count,
            norm_negative_sum = sentiment_negative_count - sdg_not_relevant_negative_count,
            ) %>%
  group_by(isin) %>%
  mutate(norm_positive_sum = lead(norm_positive_sum,1),norm_negative_sum = lead(norm_negative_sum,1)) %>% 
  transmute(date,isin,ret,MKT_CAP,
    norm_positive_sum = replace_na(norm_positive_sum,0),
    norm_negative_sum = replace_na(norm_negative_sum,0),
    roll_mean = across(!c(date,ret,MKT_CAP),
                      ~ rollmean(.x, k = y, fill = NA, align = "right"))) %>%
  unnest(roll_mean,names_sep = "_") %>% arrange(isin) %>%
  group_by(isin) %>%
    mutate(
    roll_mean_positive = na_if(roll_mean_norm_positive_sum, 0), 
    roll_mean_negative = na_if(roll_mean_norm_negative_sum, 0),
    #roll_mean_positive = roll_mean_norm_positive_sum, 
    #roll_mean_negative = roll_mean_norm_negative_sum,
    positive_threshold = mean(roll_mean_positive,na.rm = TRUE) + z*sd(roll_mean_positive,na.rm = TRUE),
    negative_threshold = mean(roll_mean_negative,na.rm = TRUE) + z*sd(roll_mean_negative,na.rm = TRUE)
    ) %>%
  group_by(date,isin) %>%
  # Make sure that we only calculate the cutoff value of periods with actual observations. 
  mutate(
    neg_event = 
    case_when(roll_mean_negative > negative_threshold & roll_mean_negative > 10 #& roll_mean_negative > 0.5*roll_mean_norm_positive_sum 
              ~ 1, 
              TRUE ~ 0)
    ) 

############################
# Negative events:
############################

# Add lagged values of the events to sort whether an event has happened in the last X months. 
  for (i in 1:12) {
  long_term_data_monthly[paste0("-",i)] <- long_term_data_monthly %>% group_by(isin) %>% transmute(t = lag(neg_event,i)) %>% ungroup() %>% select(-isin)
  }

long_term_data_negative = long_term_data_monthly %>% select(date,isin,ret,MKT_CAP,contains("-")) %>%
  pivot_longer(!c(date,isin,ret,MKT_CAP), names_to = "period", values_to = "event") %>%
  mutate(period = as.numeric(period)) %>% 
  filter(event == '1') %>%
    # Remove duplicate rows of the returns in case one ISIN pops up on several dates. 
  group_by(date,isin) %>% distinct(ret, .keep_all = TRUE)


# x Month portfolio returns:

list = list(1,4,8,12)
alpha_negative <- data.frame(matrix(nrow=4, ncol=8)) 
colnames(alpha_negative)<-c("Alpha","t","p","r","ret","mkt","N","model")

for (x in 1:length(list)) {
  
  i = list[[x]]

long_term_data_M_negative = long_term_data_negative %>%
     # Only include the last 1 months:
     filter(period >= -i) %>% 
     na.omit() %>%
     group_by(date) %>% 
  summarise(
    ret =mean(ret), 
    #ret = weighted.mean(ret,MKT_CAP),
    n = n_distinct(isin)) %>%
  left_join(dev_5factors_monthly , by = "date") 

# Calculate alpha of portfolios:
  
alpha_M_negative = long_term_data_M_negative %>% 
  summarise(alpha = tidy(lm(ret - RF ~ 1 + mkt_excess + SMB + HML + RMW + CMA, weights = n))$estimate[1],
             t = tidy(lm(ret - RF ~ 1 + mkt_excess + SMB + HML + RMW + CMA, weights = n))$statistic[1],
             p = tidy(lm(ret - RF ~ 1 + mkt_excess + SMB + HML + RMW + CMA, weights = n))$p.value[1],
            r = glance(lm(ret - RF ~ 1 + mkt_excess + SMB + HML + RMW + CMA, weights = n))$r.squared,
            ret = 100*mean(12*ret,na.rm = TRUE),
            mkt = 100*mean(12*mkt_excess, na.rm = TRUE),
             N = mean(n)) %>%
   mutate(model = paste0(i,"M_",y,"_",z)) 

alpha_negative[x,] = alpha_M_negative

}
emptylist_neg_1[[z]] = alpha_negative
}
emptylist_neg_2[[y]] = emptylist_neg_1
}

emptylist_neg_2

print(xtable(as.data.frame(emptylist_neg_2[[1]][1]),type="latex",
             digits=c(0,4,4,4,4,4,2,0,2),
             hline.after=c(-1,0),
             floating=FALSE,latex.environments=NULL),
      include.rownames=FALSE,
      include.colnames = TRUE) %>% 
  write(file="alpha_negative_nasdaq_FF5_high.tex")

tidy(summary(lm(ret - RF ~ 1 + mkt_excess + SMB + HML + RMW + CMA, weights = n, data = long_term_data_M_negative)))


model = lm(ret - RF ~ 1 + mkt_excess + SMB + HML + RMW + CMA, weights = n, data = long_term_data_M_negative)

summary(model)

library(lmtest)
library(sandwich)

coeftest(model, vcov = vcovHC(model, type = 'HC0'))

lmtest::bptest(model)
lmtest::dwtest(model)
lmtest::bgtest(model, order = 1)

```



################ 
Short term abnormal return after an individual event - Daily 
################

```{r}


short_term_data_daily = data_daily %>% 
  right_join(isin_nasdaq, by = "isin") %>%
  mutate(date = as.Date(date)) %>% 
  right_join(index_daily %>% select(date, mkt_excess), by = "date") %>%
  left_join(mkt_cap %>% select(isin,MKT_CAP), by = "isin")

# 
#  plan(multisession, workers = availableCores())
# 
#  short_term_data_nested <- short_term_data_daily %>% mutate(W_RETURN = value) %>% 
#    ungroup()  %>%
#    select(date,isin,W_RETURN,mkt_excess) %>% na.omit() %>%
#    nest(data = -c(isin)) %>%
#    mutate(beta = future_map(
#      data, ~ roll_capm_estimation(., window = 100, min_obs = 50, period = "day")
#    )) %>%
#    unnest(c(beta)) %>%
#    na.omit(beta,alpha) %>% select(-data)
#                              
#    # Write to CSV to keep in nice format:
#         short_term_data_nested %>%
#             write.csv(file = "C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/nested_daily_for_capm_nasdaq.csv")

 betas <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/nested_daily_for_capm_nasdaq.csv", sep = ",") %>%
   select(-X) %>%
   mutate(date = as.Date(date))

 
short_term_data <- betas %>% 
  left_join(short_term_data_daily, by = c("date","isin")) %>%
  group_by(isin) %>%
  mutate(
      pred_ret = alpha + beta*mkt_excess,
      abnormal_ret = value - pred_ret,
      norm_positive_sum = sentiment_positive_count - sdg_not_relevant_positive_count,
      norm_negative_sum = sentiment_negative_count - sdg_not_relevant_negative_count#,
  #     
  #     roll_sum = across(!c(date,sdg_not_relevant_positive_count,sdg_not_relevant_negative_count,beta,alpha,W_RETURN,negative_sum,
  #                     positive_sum,mkt_excess,pred_ret,abnormal_ret,sentiment_positive_count,sentiment_negative_count),
  #                     ~ rollsum(.x, k = 5, fill = NA, align = "right"))) %>% 
  # unnest(roll_sum,names_sep = "_") %>%
     
  ) %>%
  select(-c(sdg_not_relevant_positive_count,sdg_not_relevant_negative_count,sdg_not_relevant_neutral_count,firm,match_names:global_unique_sources,))

remove(short_term_data_daily,short_term_data_nested,betas,short_term_data_sdg,sentiment_daily,data_daily,sxxp_index_daily,msci_index_daily)

# Create columns that represent the lagged and leading values in relation to the event date:


for (i in 1:10) {
  short_term_data[paste0("-",i)] <- short_term_data %>% group_by(isin) %>% transmute(t = lag(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
  short_term_data[paste0("+",i)] <- short_term_data %>% group_by(isin) %>% transmute(t = lead(abnormal_ret,i)) %>% ungroup() %>% select(-isin)
}

short_term_data_event = short_term_data %>%
  group_by(isin) %>%
  mutate(
    #norm_positive_sum = na_if(norm_positive_sum, 0),
    #norm_negative_sum = na_if(norm_negative_sum, 0),
    norm_positive_sum = replace_na(norm_positive_sum,0),
    norm_negative_sum = replace_na(norm_negative_sum,0),
    positive_threshold = mean(norm_positive_sum,na.rm = TRUE) + 1*sd(norm_positive_sum,na.rm = TRUE),
    negative_threshold = mean(norm_negative_sum,na.rm = TRUE) + 1*sd(norm_negative_sum,na.rm = TRUE)
    ) %>%
  rename("0" = abnormal_ret) %>%
  
   select(-c("value",
             #"positive_threshold","negative_threshold","norm_positive_sum","norm_negative_sum",
              "sentiment_negative_count","sentiment_positive_count",
             #"roll_mean_norm_negative_sum","roll_mean_norm_positive_sum", "roll_sum_norm_negative_sum","roll_sum_norm_positive_sum"
             )) %>%

  group_by(date,isin) %>%
  # Make sure that we only calculate the cutoff value of periods with actual observations. 
  mutate(pos_event = 
    case_when(norm_positive_sum > positive_threshold & norm_positive_sum > 10 & norm_positive_sum > 2*norm_negative_sum ~ 1,
              TRUE ~ 0),
    neg_event = 
    case_when(norm_negative_sum > negative_threshold & norm_negative_sum > 10 & norm_negative_sum > 2*norm_positive_sum ~ 1, 
              TRUE ~ 0)
    ) 

# Write to CSV to keep in nice format:
       short_term_data_event %>%
           write.csv(file = "C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/short_term_data_event_nasdaq.csv")

        short_term_data_event <- read.csv("C:/Users/Marti/OneDrive - University of Copenhagen/KU/Speciale/Data behandling/short_term_data_event_nasdaq.csv", sep = ",") %>%
   select(-X) %>%
   mutate(date = as.Date(date))

#Splitting events into positive and negative, and cleaning the data

# Positive 

        
positive_index = short_term_data_event %>% 
  select(date,isin,MKT_CAP,"0",contains("-"),contains("+"),pos_event) %>% 
  select(-c(contains("neg_event_"),contains("pos_event_"))) %>%
  filter(pos_event == 1) %>%
  select(-pos_event) %>%
  pivot_longer(!c(date,isin,MKT_CAP), names_to = "type", values_to = "value") %>%
  na.omit(value)  %>%
  mutate(type = factor(type, levels = c("-10", "-9","-8","-7","-6","-5","-4","-3","-2","-1",
                                        "0", "+1", "+2","+3","+4","+5","+6","+7","+8","+9","+10"))) %>%
  arrange(isin,date,type) %>%
  group_by(isin,date)


positive_index_10 = positive_index %>% 
  mutate(csum = cumsum(value)) %>%
  group_by(type) %>%
  summarise(
     AAR = weighted.mean(value,MKT_CAP),
    CAAR = weighted.mean(csum,MKT_CAP),
    SE_AAR = sd(value, na.rm = TRUE)/sqrt(n()),
    n = n(),
    SE_CAAR = sd(csum, na.rm = TRUE)/sqrt(n())
    )



positive_index_10 %>% select(type,AAR,CAAR) %>%
    pivot_longer(!c(type), names_to = "metric", values_to = "ret") %>%
    left_join(
     positive_index_10 %>% select(type, "AAR" = SE_AAR, "CAAR" = SE_CAAR) %>% 
       pivot_longer(!c(type), names_to = "metric", values_to ="SE"), by = c("metric","type")) %>%
    mutate( 
     low = ret - 1.96*SE,
     high = ret + 1.96*SE
   ) %>%
    group_by(metric) %>%
    ggplot(aes(x = type, y = ret,colour = metric, group = metric)) +
    geom_ribbon(data = ~ filter(.x, metric %in% c("CAAR", "AAR")),
                  aes(ymin = low, ymax = high, fill = metric),
                 alpha=0.3, linetype="dashed", show.legend = FALSE) +
    
    geom_line(data = ~ filter(.x, metric %in% c("CAAR", "AAR")),
              linewidth=1) + 
    theme_bw() +
    theme(legend.title = element_blank(),
          legend.position="top",
          legend.direction = "horizontal") +
  scale_y_continuous(name ="Abnormal return",
                     breaks = pretty_breaks(),
                     labels = scales::percent) +
  scale_x_discrete(name = "Relative time",breaks=c("-10","-5","0","+5","+10"))
ggsave("ST_positive_all_CI_nasdaq.png")

### negative 

negative_index = short_term_data_event_nasdaq %>% 
  select(date,isin,MKT_CAP,"0",contains("-"),contains("+"),neg_event) %>% 
  select(-c(contains("pos_event_"),contains("neg_event_"))) %>%
  filter(neg_event == 1) %>%
  select(-neg_event) %>%
  pivot_longer(!c(date,isin,MKT_CAP), names_to = "type", values_to = "value") %>%
  na.omit(value)  %>%
  mutate(type = factor(type, levels = c("-10", "-9","-8","-7","-6","-5","-4","-3","-2","-1",
                                        "0", "+1", "+2","+3","+4","+5","+6","+7","+8","+9","+10"))) %>%
  arrange(isin,date,type) %>%
  group_by(isin,date)

negative_index_10 = negative_index %>% 
  mutate(csum = cumsum(value)) %>%
  group_by(type) %>%
  summarise(
    AAR = weighted.mean(value,MKT_CAP),
    CAAR = weighted.mean(csum,MKT_CAP),
    n = n(),
    SE_AAR = sd(value, na.rm = TRUE)/sqrt(n()),
    SE_CAAR = sd(csum, na.rm = TRUE)/sqrt(n())
    )


negative_index_10 %>% select(type,AAR,CAAR) %>%
    pivot_longer(!c(type), names_to = "metric", values_to = "ret") %>%
    left_join(
     negative_index_10 %>% select(type, "AAR" = SE_AAR, "CAAR" = SE_CAAR) %>% 
       pivot_longer(!c(type), names_to = "metric", values_to ="SE"), by = c("metric","type")) %>%
    mutate( 
     low = ret - 1.96*SE,
     high = ret + 1.96*SE
   ) %>%
    group_by(metric) %>%
    ggplot(aes(x = type, y = ret,colour = metric, group = metric)) +
    geom_ribbon(data = ~ filter(.x, metric %in% c("CAAR", "AAR")),
                  aes(ymin = low, ymax = high, fill = metric),
                 alpha=0.1, linetype="dashed", show.legend = FALSE) +
    
    geom_line(data = ~ filter(.x, metric %in% c("CAAR", "AAR")),
              linewidth=1) + 
    theme_bw() +
    theme(legend.title = element_blank(),
          legend.position="top",
          legend.direction = "horizontal") +
  scale_y_continuous(name ="Abnormal return",
                     breaks = pretty_breaks(),
                     labels = scales::percent) +
  scale_x_discrete(name = "Relative time",breaks=c("-10","-5","0","+5","+10"))
ggsave("ST_negative_all_CI_nasdaq.png")

### 10 days around the event: 

###################################

# ESG:


positive_index_10 = positive_index %>% 
  left_join(ESG_RR, by = "isin") %>%
  mutate(csum = cumsum(value)) %>%
  group_by(type,ESG_risk_category) %>% na.omit() %>%
  summarise(
    AAR = mean(value),
    n = n(),
    CAAR = mean(csum),
    SE_AAR = sd(value, na.rm = TRUE)/sqrt(n()),
    SE_CAAR = sd(csum, na.rm = TRUE)/sqrt(n())
    )

positive = positive_index_10 %>%
    transmute(type, ESG_risk_category,AAR,CAAR) %>%
    pivot_longer(!c(type,ESG_risk_category), names_to = "metric", values_to = "ret") %>%
    left_join(
     positive_index_10 %>% select(type,ESG_risk_category, "AAR" = SE_AAR, "CAAR" = SE_CAAR) %>% 
       pivot_longer(!c(type,ESG_risk_category), names_to = "metric", values_to = "SE"), by = c("metric","type","ESG_risk_category")
     ) %>%
    mutate(
     low = ret - 1.96*SE,
     high = ret + 1.96*SE
   )

positive %>% filter(metric == 'CAAR') %>%
    mutate(ESG_risk_category = factor(ESG_risk_category, levels = c("Low", "Medium","High"))) %>% 
    group_by(ESG_risk_category) %>%
    ggplot(aes(x = type, y = ret,colour = ESG_risk_category, group = ESG_risk_category)) +
    geom_ribbon(aes(ymin = low, ymax = high, fill = ESG_risk_category),
                 alpha=0.1, linetype="dashed", show.legend = FALSE) +
  geom_line(size=1) +
    theme_bw() +
   theme(legend.title = element_blank(),
          legend.position="top",
          legend.direction = "horizontal") +
  scale_y_continuous(name ="Abnormal return",
                     breaks = pretty_breaks(),
                     labels = scales::percent) +
  scale_x_discrete(name = "Relative time", breaks = c("-10","-5","0","+5","+10"))
  ggsave("ST_positive_ESG_nasdaq.png")



  
  #######
  
negative_index_10 = negative_index %>% 
  left_join(ESG, by = "isin") %>%
  mutate(csum = cumsum(value)) %>%
  group_by(type,ESG_risk_category) %>% na.omit() %>%
  summarise(
    AAR = weighted.mean(value),
    n = n(),
    CAAR = mean(csum),
    SE_AAR = sd(value, na.rm = TRUE)/sqrt(n()),
    SE_CAAR = sd(csum, na.rm = TRUE)/sqrt(n())
    )

negative = negative_index_10 %>%
    transmute(type, ESG_risk_category,AAR,CAAR) %>%
    pivot_longer(!c(type,ESG_risk_category), names_to = "metric", values_to = "ret") %>%
    left_join(
     negative_index_10 %>% select(type,ESG_risk_category, "AAR" = SE_AAR, "CAAR" = SE_CAAR) %>% 
       pivot_longer(!c(type,ESG_risk_category), names_to = "metric", values_to = "SE"), by = c("metric","type","ESG_risk_category")
     ) %>%
    mutate(
     low = ret - 1.96*SE,
     high = ret + 1.96*SE
   )

negative %>% filter(metric == 'CAAR') %>%
    mutate(ESG_risk_category = factor(ESG_risk_category, levels = c("Low", "Medium","High"))) %>% 
    group_by(ESG_risk_category) %>%
    ggplot(aes(x = type, y = ret,colour = ESG_risk_category, group = ESG_risk_category)) +
    geom_ribbon(aes(ymin = low, ymax = high, fill = ESG_risk_category),
                 alpha=0.3, linetype="dashed", show.legend = FALSE) +
  geom_line(size=1) +
    theme_bw() +
   theme(legend.title = element_blank(),
          legend.position="top",
          legend.direction = "horizontal") +
  scale_y_continuous(name ="Abnormal return",
                     breaks = pretty_breaks(),
                     labels = scales::percent) +
  scale_x_discrete(name = "Relative time", breaks = c("-10","-5","0","+5","+10"))
  ggsave("ST_negative_ESG_nasdaq.png")
 

```


### Negative news
```{r}

rolling_means = list(1,2,3)
SDs = list(1,2,3)
emptylist_neg_1 = list()
emptylist_neg_2 = list()


for (y in 1:length(rolling_means)) {
  
  for (z in 1:length(SDs)) { 

long_term_data_monthly <- data %>%
  group_by(isin) %>%
  transmute(date,isin,ret,MKT_CAP,
    norm_negative_sum = replace_na(norm_negative_sum,0),
    roll_mean = across(!c(date,ret,MKT_CAP),
                      ~ rollmean(.x, k = y, fill = NA, align = "right"))) %>%
  unnest(roll_mean,names_sep = "_") %>% arrange(isin) %>%
  group_by(isin) %>%
    mutate(
    roll_mean_negative = na_if(roll_mean_norm_negative_sum, 0),
    negative_threshold = mean(roll_mean_negative,na.rm = TRUE) + z*sd(roll_mean_negative,na.rm = TRUE)
    ) %>%
  group_by(date,isin) %>%
  # Make sure that we only calculate the cutoff value of periods with actual observations. 
  mutate(
    neg_event = 
    case_when(roll_mean_negative > negative_threshold & roll_mean_negative > 5 #& roll_mean_negative > 0.5*roll_mean_norm_positive_sum 
              ~ 1, 
              TRUE ~ 0)
    ) 

############################
# Negative events:
############################

# Add lagged values of the events to sort whether an event has happened in the last X months. 
  for (i in 1:24) {
  long_term_data_monthly[paste0("-",i)] <- long_term_data_monthly %>% group_by(isin) %>% transmute(t = lag(neg_event,i)) %>% ungroup() %>% select(-isin)
  }

long_term_data_negative = long_term_data_monthly %>% select(date,isin,ret,MKT_CAP,contains("-")) %>%
  pivot_longer(!c(date,isin,ret,MKT_CAP), names_to = "period", values_to = "event") %>%
  mutate(period = as.numeric(period)) %>% 
  filter(event == '1') %>%
    # Remove duplicate rows of the returns in case one ISIN pops up on several dates. 
  group_by(date,isin) %>% distinct(ret, .keep_all = TRUE)

# x Month portfolio returns:

list = list(1,3,6,12,18,24)
alpha_negative <- data.frame(matrix(nrow=4, ncol=10)) 
colnames(alpha_negative)<-c("Alpha","t","p", "r","return","index_return", "ave_N","holding_period","rolling_mean","SD_thres")
  
for (x in 1:length(list)) {
  
  i = list[[x]]

long_term_data_M_negative = long_term_data_negative %>%
  # Only include the last 1 months:
  filter(period >= -i) %>% 
  group_by(date) %>%
  summarise(ret = weighted.mean(ret,MKT_CAP),
            n = n_distinct(isin)) %>%
  left_join(FF3, by = "date") %>% na.omit()

# Calculate alpha of portfolios:
  
alpha_M_negative = long_term_data_M_negative %>% 
   summarise(alpha = tidy(lm(ret - RF ~ 1 + mkt_excess_ret + SMB + HML, weights = n))$estimate[1],
             t = tidy(lm(ret - RF ~ 1 + mkt_excess_ret + SMB + HML, weights = n))$statistic[1],
             p = tidy(lm(ret - RF ~ 1 + mkt_excess_ret + SMB + HML, weights = n))$p.value[1],
              r = glance(lm(ret - RF ~ 1 + mkt_excess_ret + SMB + HML, weights = n))$r.squared,
             return = 100*mean((52*ret-RF)),
             index_return = 100*mean(52*mkt_excess_ret),
             ave_N = mean(n)) %>%
   mutate(holding_period = paste0(i,"M"),
          rolling_mean = y,
          SD_thres = z
          ) 

alpha_negative[x,] = alpha_M_negative

}
emptylist_neg_1[[z]] = alpha_negative
}
emptylist_neg_2[[y]] = emptylist_neg_1
}


print(xtable(as.data.frame(emptylist_neg_2[[5]][3]),type="latex",
             digits=c(0,4,2,3,2,0,1,1,1,1),
             hline.after=c(-1,0),
             floating=FALSE,latex.environments=NULL),
      include.rownames=FALSE,
      include.colnames = TRUE) %>% 
  write(file="alpha_negative_FF3_TEST.tex")


```

